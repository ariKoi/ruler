---
title: "ruler"
author: "Ari Koitsanos"
date: "1/3/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

# An envelope representation of prediction error 

In a regression task, once a model or algorithm has been implemented and tested on out of sample data, we obtain the pair of actual and predicted response variable values, noted as $y$ and $\hat{y}$ respectively. In most cases we would expect $y$ and $\hat{y}$ values not to be too far away from each other. Now if we treat this pair as if they were a pair of nearly identical frequencies $f_y$ and $f_\hat{y}$ respectively, we can describe two closely spaced sinusoids as: 

$$\boldsymbol{cos(2*π*f_y*t)}$$
$$\boldsymbol{cos(2*π*f_\hat{y}*t)}$$
where for simplicity we assume that the phase is zero. If we were then to add these two sinusoids we would obtain the beat note

$$\boldsymbol{e(t) = cos(2*π*f_y*t) + cos(2*π*f_\hat{y}*t)}$$
which can also be expressed as a product of two familiar quantities in statistical science: The mean and difference values or in the frequency analogy we are using here, the center and deviation frequencies. Those are defined as:

$$\boldsymbol{f_c = 1/2 * (f_y + f_\hat{y})}$$
$$\boldsymbol{f_Δ = 1/2 * (f_\hat{y} - f_y)}$$
$f_Δ$ is usually much smaller than $f_c$. 

In the usual statistical interpretation these correspond to the mean 

$$\boldsymbol{mean = 1/2 * (\hat{y} + y)}$$
and the residual or difference 

$$\boldsymbol{residual = 1/2 * (y - \hat{y})}$$
with the only difference being that we take half of the residual value in the frequency analogy. Similarly, the residual is usually much smaller than the mean value. In addition these two quantities form the basis of Tukey's classic mean-difference plot or what's known in biomedical applications as the Bland-Altman plot. Such plots have been widely used to examine the error residuals produced after a model has been built and applied on out of sample data, and as a tool to help with calibrating further the predictions in order to improve the model's predictive performance. 

Continuing with the frequency analogy, McClellan et al. show how using the inverse Euler formula, the sum of the two sinusoids can be converted into a product of sinusoids involving the center frequency and deviation frequency: 

$$\begin{eqnarray} e(t) &=& Re\{e^{j2πf_\hat{y}t}\} + Re\{e^{j2πf_yt}\} \\ &=& Re\{e^{j2π(f_c-f_Δ)t}\} + Re\{e^{j2π(f_c+f_Δ)t}\} \\ &=& Re\{e^{j2πf_ct} \cdot (e^{-j2πf_Δt} + e^{j2πf_Δt})\} && \text{(using the inverse Euler formula we get)} \\ &=& Re\{e^{j2πf_ct} \cdot (2\cdot cos(2πf_Δt))\} \\ &=& 2 \cdot cos(2πf_Δt) \cdot cos(2πf_ct) && \text{(result)} \end{eqnarray}$$

In this way the residual error e (i.e. sum of sinusoids mimics the negative and positive values of prediction error) can be thought of being expressed as a product of two sinusoids involving a deviation frequency and a center frequency with the frequency notion having replaced the model's true and predicted value pairs. Also note that in this formulation the residual error is a function of time, as is the case for any signal. This allows us to view errors as signals evolving through time and to thus visualize them in a different way from common practice. In addition, as shown below, this allows us to attempt the construction of features that could potentially help in improving the model's predictive performance (actually to generate new response values per each predictor vector as one would observe in a controlled experiment).  

# Envelope Calculation And Plotting 

As an example, suppose that we would like to model the maximum heart rate value during physical exercise based on a variety of predictor variables such as a person's breathing frequency, tidal volume and energy expenditure. After we have fitted a model or implemented an algorithm we obtain the out of sample residuals. One of the largest residuals obtained correspond to a case with an actual heart rate of 180 beats per minute while the predicted value was 140 beats per minute. Hence $Y$ = 180 and $\hat{y}$ = 140. Such heart rate values are quite high and less likely, and as a result they can be difficult to predict. In this case the mean value of the $y$ and $\hat{y}$ pair is 160 beats per minute, while half of the out of sample residual of this pair has the value of 20 beats per minute. In the frequency analogy those units would have been Hz. Then from the above result we would obtain a representation of the prediction error as 

$$\begin{eqnarray} e(t) &=& 2 \cdot cos(2π(20)t) \cdot cos(2π(160)t) \end{eqnarray}$$

```{r}
par(mfrow = c(2,1))
te <- seq(0, 0.1, by = 1/(2*160*10)) # 0.1 secs are 100 msecs i.e. 100/1000
fdelta <- 2*cos(2*pi*20*te) 
fc <- cos(2*pi*160*te) 
plot(te*1000, fdelta, type = "l", main = "envelope components - center & deviation", xlab = "time (msecs)")
lines(te*1000, fc, col = "blue")
e <- fdelta * fc
plot(te*1000, e, type = "l", col = "magenta", main = "envelope representation", xlab = "time (msecs)")
lines(te*1000, fdelta, lty = "dashed", col = "blue")
lines(te*1000, -fdelta, lty = "dashed", col = "blue")
```

The deviation frequency (error) stretches the center frequency (mean) to a signal with time-varying amplitude. The signal envelope is rising and falling with period $1/2 * 1/f_Δ = 1/2 * 1/20 = 25\ msec$, and with a frequency of $2*f_Δ = 40\ HZ$. For e(t) the spectrum contains frequency components at +-180 Hz and +-140 Hz. The time interval between nulls (zeros) of the envelope is the 25 msec period, which is dictated by the frequency deviation $f_Δ$. 

One notion that stands out from this result, is that the error is now thought of as a signal that fluctuates with time. 

Predicting a heart rate of 140 bpms when the actual is 180, is already a way off prediction. However let's see other examples of both bad and good predictions and visually compare the results. 

## envelope and envelope_vec functions

```{r}

envelope <- function(y, yhat, te = NA, maxSecs = 0.1, plot = TRUE, both = FALSE, dev = TRUE){
  # both: plot both the components and the envelope
  # dev: plot the deviation envelope or not in the envelope representation plot
  # maxSecs: max number of seconds in calculation and plot
  # te is in msec so if non NA the values needs to be divided by 1e3 to convert to sec
  # browser()
  if (any(y<0,yhat<0)) stop("---y and yhat values can't be negative---")
  if (y == yhat) stop("---y and yhat values are the same. Zero error, really?---")
  halfDev <- abs(y - yhat)/2
  center <- mean(c(y, yhat))
  if (is.na(te)) te <- seq(0, maxSecs, by = 1/(2*10*(center+halfDev))) # in sec 
  fdelta <- 2*cos(2*pi*halfDev*te) 
  fc <- cos(2*pi*center*te) 
  e <- fdelta * fc
  if (plot) {
  ylims <- c(min(fdelta, fc), max(fdelta, fc))
  if (both){
    par(mfrow = c(2,1))
    plot(te, fdelta, type = "l", main = "envelope components - center & deviation", xlab = "time (msec)", ylim = ylims, xaxt = "n")
    x_ticks <- axis(1, at = seq(0, maxSecs, maxSecs/10), labels = 1e3*seq(0, maxSecs, maxSecs/10), cex.axis = 0.7)
    abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
    lines(te, fc, col = "blue")
    stopifnot(maxSecs >= (1/halfDev)) # "maxSecs is too short. Please increase the value."
    abline(v = seq(1/halfDev, maxSecs, 1/halfDev), lwd = 1.5, lty = 4, col = "black") # marking the deviation period
  }
  plot(te, e, type = "l", col = "magenta", main = "envelope representation", xlab = "time (msec)", ylab = NA, xaxt = "n")
  x_ticks <- axis(1, at = seq(0, maxSecs, maxSecs/10), labels = 1e3*seq(0, maxSecs, maxSecs/10), cex.axis = 0.7)
  abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
  ePeriod <- 1/(2 * halfDev)
  abline(v = seq(ePeriod, maxSecs, ePeriod), lwd = 1.5, lty = 4, col = "gray") # marking the envelope period
  if (dev == TRUE) {
   lines(te, fdelta, lty = "dashed", col = "blue")
   lines(te, -fdelta, lty = "dashed", col = "blue")  
  }
  mtext("e", side = 2, col = "blue", line = 3)
  list(
  resData = data.frame(samples = length(te), halfDev = halfDev, center = center, ePeriodSecs = ePeriod, eFreq = 1/ePeriod), 
  eRaw = tibble::as_tibble(data.frame(t = te, e = e, fdelta = fdelta, fc = fc))
  )
  } else {
    e
  }
}

### Not needed:
# envelope_vec <- function(y, yhat, te = NA, maxSecs = 0.1){
#   # vectorized version 
#   # maxSecs: max number of seconds in calculation 
#   # te is in msec so if non NA the values needs to be divided by 1e3 to convert to sec
#   # browser()
#   if (any(y<0,yhat<0)) stop("---y and yhat values can't be negative---")
#   if (any(y == yhat)) stop("---There's a case with equal y and yhat values. Zero error, really?---")
#   if (length(y) != length(yhat)) stop("---y and yhat vector lengths are not the same---")
#   halfDev <- abs(y - yhat)/2
#   center <- apply(matrix(c(y, yhat), ncol = 2), MARGIN = 1, mean)
#   max_freq <- center+halfDev
#   
#   if (is.na(te)) te <- lapply(1:length(y), function(i) seq(0, maxSecs, by = 1/(2*max_freq[i])))
#   # fdelta <- lapply(1:length(y), function(i) 2*cos(2*pi*halfDev[i]*te[[i]]))
#   # fc <- lapply(1:length(y), function(i) cos(2*pi*center[i]*te[[i]]))
#   # e <- lapply(1:length(y), function(i) fdelta[[i]] * fc[[i]]) 
#   # altogether:
#   e <- lapply(1:length(y), function(i) 2*cos(2*pi*halfDev[i]*te[[i]]) * cos(2*pi*center[i]*te[[i]]))
#   return(list(e = e, te = te))
# }
# # i.e. 
# envelope_vec(y = c(180,170,160), yhat = c(160,145,150))
###
```

```{r}
envelope(y = 180, yhat = 140, both = T) # underestimation
# Some info on upper plot: deviation frequency 20 cycles per sec (Hz) or 20 cycles per 1000 msec. Since we are plotting here only 100 msecs we can see 20 / 1000/100 = 2 cycles that occur every 1000 * 1/20 = 50 msecs. Similarly for the center frequency: 160 / 1000/10 = 16 cycles every 1000 * 1/160 = 6.25 msecs.  
# e frequency and period: 2*20 / 1000/10 = 4 cycles every 1000 * 1/(2*20) = 25 msecs. 
envelope(y = 180, yhat = 140, te = 20/1e3, plot = F)
envelope(y = 180, yhat = 140, te = 25/1e3, plot = F)
envelope(y = 180, yhat = 140, te = 28/1e3, plot = F)
envelope(y = 180, yhat = 140, te = 10.9/1e3, plot = F)

envelope(y = 180, yhat = 100, both = T) # deviation now has greater frequency while center has a bit lower; less of the center signal is captured by the deviation envelope, e.g.
envelope(y = 180, yhat = 70, both = T)
envelope(y = 180, yhat = 30, both = T)
# As prediction gets better:
envelope(y = 180, yhat = 140, both = T)
envelope(y = 180, yhat = 150, both = T) # deviation has lower frequency or envelope variation is slower
envelope(y = 180, yhat = 160, both = T)
envelope(y = 180, yhat = 170, both = T, maxSecs = 0.2)
envelope(y = 180, yhat = 175, both = T, maxSecs = 0.4)
envelope(y = 180, yhat = 179, both = T, maxSecs = 2) # and progressively more of the center signal is captured by the deviation envelope 
# Note that increasing yhat to be very close to y takes long to compute and need to increase maxSecs. 

# Remarks: Given a large maxSecs argument value, when the error is small (small frequency) then the envelope signal has a large period containing a good stretch of the center signal and having a few cycles, and when the error is large (large frequency) then the envelope has a small period containing less of the center signal with many cycles. 

# Same error in magnitude but in other direction i.e. overestimation
envelope(y = 180, yhat = 220, both = T)
envelope(y = 180, yhat = 260, both = T)
envelope(y = 180, yhat = 300, both = T)
envelope(y = 180, yhat = 600, both = T) # less and less of the deviation signal is captured by the envelope

# Extremes:
# Best possible prediction:
envelope(y = 180, yhat = 180, both = T) # 0 deviation with its whole signal within the envelope # not realistic though
# Worse possible prediction:
envelope(y = 180, yhat = 0, both = T) # center & deviation match, while there is envelope since the center is stretched 
# function should error on these cos it's either zero error so no envelope or error that is too high. (not sure what I meant with this)

# Different scale:
envelope(y = 18, yhat = 14, both = T, maxSecs = 1) # same result as with y = 180, yhat = 140, just time axis is multiplied by 10 i.e. 10*100 msecs
```

The deviation frequency signal is the one that dictates the envelope representation. As the prediction gets better, it captures more and more of the center frequency signal, while as the prediction gets worse, it captures less and less of the center frequency signal. The more the deviation signal contains the center signal through time, the better is the prediction throughout future time. 

If the unit of measurement is high relative to the error, then the envelope pattern is already delineated by the center component signal and hence the deviation signal is not needed to be shown.

```{r}
# previously: envelope(y = 180, yhat = 140, both = T) # underestimation
envelope(y = 1800, yhat = 1760, both = T, dev = F) # maxSecs = 2 takes time
# envelope(y = 1800, yhat = 1000, both = T, dev = F)
```

Compared to the previous example with the smaller measurement of 180 but same error, more of the signal is captured in this case, so such visuals are indicative of better predictive ability. 

Another way to think about this is via the signal to noise ratio, which is ten times more when compared to the previous case:
```{r}
180/(40)
1800/(40)
```

The deviation signal (error) modulates the center signal (response variable) by expanding and shrinking its amplitude. This will be useful when we want to derive features that are based on this varying amplitude center signal, in order to improve the predictive performance of a selected model. 
# Deriving Response Values From The Envelope 

Below there are three tries. The second one seems to be the correct one. 

## First Approach

$$\begin{eqnarray} e(t) &=& Re\{e^{j2πf_\hat{y}t}\} + Re\{e^{j2πf_yt}\} &=& cos(2πf_\hat{y}t) + cos(2πf_yt) \end{eqnarray}$$

```{r}

yNew <- function(t, y, yhat){
  # t is in msec
  # acos(e_t - cos(2*pi*yhat)) / (2*pi)
  # browser()
  e <- et(t, y, yhat) # I originally called the envelope function as et # haven't changed it here
  # acos(e - cos(2*pi*yhat*t/1e3)) / (2*pi*t/1e3) + yhat # y in resid
  y - acos(e - cos(2*pi*y*t/1e3)) / (2*pi*t/1e3) # yhat in resid
  # y - acos(e - cos(2*pi*y*t/1e3)) / (2*pi*t/1e3) # yhat in resid
}

yNew(20, 180, 140)
yNew(22, 180, 140)
yNew(30, 180, 140)
yNew(40, 180, 140)
yNew(50, 180, 140)

yhatExample <- 140
plot(seq(0.001, 525, length.out = 21*2*40+1), sapply(seq(0.001, 525, length.out = 21*2*40+1), function(i) yNew(i, 180, yhatExample)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = seq(0.001, 525, length.out = 100), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 25, lty = "dotted")
#
plot(seq(0.001, 50, length.out = 2*40+1), sapply(seq(0.001, 50, length.out = 2*40+1), function(i) yNew(i, 180, yhatExample)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = seq(0.001, 50, length.out = 100), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 25, lty = "dotted")


plot(seq(25, 525, length.out = 20*2*40+1), sapply(seq(25, 525, length.out = 20*2*40+1), function(i) yNew(i, 180, yhatExample)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = seq(25, 525, length.out = 100), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 25, lty = "dotted")
#
plot(seq(25, 50, length.out = 2*40+1), sapply(seq(25, 50, length.out = 2*40+1), function(i) yNew(i, 180, yhatExample)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = seq(25, 50, length.out = 100), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 25, lty = "dotted")


```

## Second Approach

$$\begin{eqnarray} e(t) &=& 2 \cdot cos(2πf_Δt) \cdot cos(2πf_ct) && \text{(result)} \end{eqnarray}$$

```{r}
yNew2 <- function(t, y, yhat){
  # t is in msec
  # browser()
  center <- mean(c(y, yhat))
  e <- et(t, y, yhat) # I originally called the envelope function as et # haven't changed it here
  residNew <- acos(e / (2 * cos(2*pi*center*t/1e3))) / (pi*t/1e3) # fdelta is half of the residual
  if (y>yhat){
   y - residNew 
  } else {
    y + residNew
  }
}

yNew2(0, 180, 140) # NaN
yNew2(0.0001, 180, 140) 
yNew2(10, 180, 140)
yNew2(20, 180, 140)
yNew2(22, 180, 140)
yNew2(30, 180, 140)
yNew2(40, 180, 140)
yNew2(50, 180, 140)
yNew2(60, 180, 140)
yNew2(70, 180, 140)
yNew2(80, 180, 140)
yNew2(90, 180, 140)
yNew2(100, 180, 140)

yhatExample <- 140 # 140, 100, 70, 30 # 150, 160, 170, 175, 179 # 220, 260, 300
# rerun below adjusting for period

plot(seq(0.001, 525, length.out = 21*2*40+1), sapply(seq(0.001, 525, length.out = 21*2*40+1), function(i) yNew2(i, 180, yhatExample)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = seq(0.001, 525, length.out = 100), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 25, lty = "dotted")
#
plot(seq(0.001, 50, length.out = 2*40+1), sapply(seq(0.001, 50, length.out = 2*40+1), function(i) yNew2(i, 180, yhatExample)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = seq(0.001, 50, length.out = 100), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 25, lty = "dotted")


# 179 : the bootstrap assumes perfect prediction

yhatExample <- 160
plot(seq(0.001, 525, length.out = 21*2*20+1), sapply(seq(0.001, 525, length.out = 21*2*20+1), function(i) yNew2(i, 180, yhatExample)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = seq(0.001, 525, length.out = 100), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 50, lty = "dotted")
#
plot(seq(0.001, 75, length.out = 2*20+1), sapply(seq(0.001, 75, length.out = 2*20+1), function(i) yNew2(i, 180, yhatExample)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = seq(0.001, 75, length.out = 100), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 50, lty = "dotted")

yhatExample <- 220
plot(seq(0.001, 525, length.out = 21*2*40+1), sapply(seq(0.001, 525, length.out = 21*2*40+1), function(i) yNew2(i, 180, yhatExample)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = seq(0.001, 525, length.out = 100), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 25, lty = "dotted")
#
plot(seq(0.001, 50, length.out = 2*40+1), sapply(seq(0.001, 50, length.out = 2*40+1), function(i) yNew2(i, 180, yhatExample)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = seq(0.001, 50, length.out = 100), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 25, lty = "dotted")

```

## Third Approach

$$\begin{eqnarray} e(t) &=& 2 \cdot cos(2πf_Δt) \cdot cos(2πf_ct) && \text{(result)} \end{eqnarray}$$

```{r}
yNew3 <- function(t, y, yhat){ # now solving for center since this is the signal whose amplitude is modulated
  # t is in msec
  # browser()
  halfDev <- abs(y - yhat)/2
  e <- et(t, y, yhat) # I originally called the envelope function as et # haven't changed it here
  centerNew <- acos(e / (2 * cos(2*pi*halfDev*t/1e3))) / (2*pi*t/1e3)
  # y - centerNew # yhat
  # modulated center frequency is 2*halfDev=2*20=40 Hz which is the original residual value
  # centerNew is like a residual value since it is stretched to follow the deviation signal. Center had a frequency of 160 and now it has a frequency of 40 i.e. the residual value.
  centerNew + yhat # y
}

yNew3(0, 180, 140) # NaN
yNew3(0.0001, 180, 140) 
yNew3(10, 180, 140)
yNew3(20, 180, 140)
yNew3(22, 180, 140)
yNew3(30, 180, 140)
yNew3(40, 180, 140)
yNew3(50, 180, 140)
yNew3(60, 180, 140)
yNew3(70, 180, 140)
yNew3(80, 180, 140)
yNew3(90, 180, 140)
yNew3(100, 180, 140)

yhatExample <- 140 # 140, 100, 70, 30 # 150, 160, 170, 175, 179 # 220, 260, 300
# rerun below adjusting for period

plot(seq(0.001, 525, length.out = 21*2*40+1), sapply(seq(0.001, 525, length.out = 21*2*40+1), function(i) yNew3(i, 180, yhatExample)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = seq(0.001, 525, length.out = 100), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 25, lty = "dotted")
#
plot(seq(0.001, 50, length.out = 2*40+1), sapply(seq(0.001, 50, length.out = 2*40+1), function(i) yNew3(i, 180, yhatExample)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = seq(0.001, 50, length.out = 100), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 25, lty = "dotted")
abline(h = 150, lty = "dotted", col = "purple")


# 179 : the bootstrap assumes perfect prediction (it always resamples the same y value)
```

## Selected Approach - Further exploration and coding 

### yvalue and response functions

The yNew2 approach seems the correct one since by keeping the center fixed and then substracting it from the e value we obtain the effect of the envelope where the original center signal is stretched towards the half-deviation signal. Hence I recode it here: 

$$\begin{eqnarray} e(t) &=& 2 \cdot cos(2πf_Δt) \cdot cos(2πf_ct) && \text{(result)} \end{eqnarray}$$

```{r}
# yvalue <- yNew2 # as in y Enveloped
yvalue <- function(y, yhat, te) {
    # te is in msec so the given argument value needs to be divided by 1e3 to convert to sec
    center <- mean(c(y, yhat))
    e <- envelope(y, yhat, te, plot = FALSE)
    residNew <- acos(e / (2 * cos(2*pi*center*te))) / (pi*te) # fdelta is half of the residual i.e. when solving for fdelta, its 2*pi*te part is multiplied by 2 so that fdelta becomes the residual
    if (y>yhat){
     y - residNew 
    } else {
      y + residNew
    }
}

### Not needed:
# yvalue_vec <- function(y, yhat, ...) {
#     # vectorized version
#     # y, yhat are vectors containing the response and predicted values in same order
#     # browser()
#     if (any(y<0,yhat<0)) stop("---y and yhat values can't be negative---")
#     if (any(y == yhat)) stop("---There's a case with equal y and yhat values. Zero error, really?---")
#     if (length(y) != length(yhat)) stop("---y and yhat vector lengths are not the same---")
#     center <- apply(matrix(c(y, yhat), ncol = 2), MARGIN = 1, mean)
#     e <- envelope_vec(y, yhat, ...) # uses default NA value for te
#     residNew <- lapply(1:length(y), function(i) acos(e$e[[i]] / (2 * cos(2*pi*center[i]*e$te[[i]]))) / (pi*e$te[[i]]))
#     yenv <- lapply(1:length(y), function(i) {
#       if (y[i]>yhat[i]){
#      y[i] - residNew[[i]] 
#     } else {
#       y[i] + residNew[[i]]
#     }
#     })
#     return(yenv)
# }
# # i.e.
# yvalue_vec(y = c(180,170,160), yhat = c(160,145,150))
# yvalue_vec(y = c(180,170,160), yhat = c(160,145,150), maxSecs = 0.2)
###

yvalue(180, 140, 0) # NaN # At time zero the calculation does not make sense since this time point corresponds to the observed response value. We need to move forward in time even slightly to start obtaining response values. 
yvalue(180, 140, 0.0001/1e3) 
yvalue(180, 140, 10/1e3)
yvalue(180, 140, 20/1e3)
yvalue(180, 140, 22/1e3)
yvalue(180, 140, 30/1e3)
yvalue(180, 140, 40/1e3)
yvalue(180, 140, 50/1e3)
yvalue(180, 140, 60/1e3)
yvalue(180, 140, 70/1e3)
yvalue(180, 140, 80/1e3)
yvalue(180, 140, 90/1e3)
yvalue(180, 140, 100/1e3)
# 
yvalue(0.8, 0.6, 7000/1e3)
```

```{r}
yhatExample <- 140 # 140, 100, 70, 30 # 150, 160, 170, 175, 179 # 220, 260, 300

plot(seq(0, 200/1e3, by = 1/(2*10*180)), sapply(seq(0, 200/1e3, by = 1/(2*10*180)), function(i) yvalue(180, yhatExample, i)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = round(seq(0, 200/1e3, 200/1e3/10), 2), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 25/1e3, lty = "dotted")
#
plot(seq(0, 50/1e3, by = 1/(2*10*180)), sapply(seq(0, 50/1e3, by = 1/(2*10*180)), function(i) yvalue(180, yhatExample, i)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = round(seq(0, 50/1e3, 50/1e3/10), 2), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 25/1e3, lty = "dotted")
# 179 : the bootstrap assumes perfect prediction

yhatExample <- 160
plot(seq(0, 200/1e3, by = 1/(2*10*180)), sapply(seq(0, 200/1e3, by = 1/(2*10*180)), function(i) yvalue(180, yhatExample, i)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = round(seq(0, 200/1e3, 200/1e3/10), 2), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 50/1e3, lty = "dotted")
#
plot(seq(0, 75/1e3, by = 1/(2*10*180)), sapply(seq(0, 75/1e3, by = 1/(2*10*180)), function(i) yvalue(180, yhatExample, i)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = round(seq(0, 75/1e3, 75/1e3/10), 2), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 50/1e3, lty = "dotted")

yhatExample <- 220
plot(seq(0, 200/1e3, by = 1/(2*10*180)), sapply(seq(0, 200/1e3, by = 1/(2*10*180)), function(i) yvalue(180, yhatExample, i)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = round(seq(0, 200/1e3, 200/1e3/10), 2), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 25/1e3, lty = "dotted")
#
plot(seq(0, 50/1e3, by = 1/(2*10*180)), sapply(seq(0, 50/1e3, by = 1/(2*10*180)), function(i) yvalue(180, yhatExample, i)), type = "l", xaxt = "n", xlab = "")
x_ticks <- axis(1, at = round(seq(0, 50/1e3, 50/1e3/10), 2), cex.axis = 0.7, las = 2)
abline(v = x_ticks, lwd = 0.7, lty = 3, col = "lightgray")
abline(h = yhatExample, lty = "dashed")
abline(v = 25/1e3, lty = "dotted")

###
response <- function(y, yhat, eperiodStart = 0, eperiods = 30, plot = TRUE, zoom_first = FALSE) {
  # eperiodStart : after how many envelope periods to begin the calculation. Default is zero. Other values are used when calculating the convergence to the observed value y. Integer number. 
  # eperiods : integer number of envelope periods to use for the time axis. Default is 30 which shows the convergence to the observed value well enough for all scales. 
  # zoom_first : Plot only the first period till convergence to y value.
  #
  # browser()
  ePeriod <- 1/abs(y-yhat)
  # length_out <- (maxSecs/ePeriod)*2*abs(y-yhat)
  halfDev <- abs(y - yhat)/2
  center <- mean(c(y, yhat))
  max_freq <- center+halfDev
  maxSecs <- eperiods * ePeriod
  minSecs <- eperiodStart * ePeriod
  stopifnot(eperiodStart < eperiods)
  te <- seq(minSecs, maxSecs, by = 1/(2*max_freq)) # in sec 
  
  # yvalue function has been defined above
  yenv <- sapply(te, function(t) yvalue(y, yhat, t)) # y enveloped
  
  te_first <- te[te > ePeriod & te <= 2*ePeriod]
  yenv_first <- yenv[which(te > ePeriod & te <= 2*ePeriod)]
  
  if (plot) {
    
    yenv_sm <- stats::supsmu(x = te[-1], y = yenv[-1], span = 0.2) # also first yenv is nan so removed
  # https://www.stat.berkeley.edu/~s133/Smooth1a.html
  # http://fmwww.bc.edu/RePEc/bocode/s/supsmooth_doc.pdf 
    
    if (zoom_first) {
      plot(te_first, yenv_first, type = "o", xaxt = "n", xlab = "time (sec)", ylab = "y (enveloped)",  yaxt = "n", bty = "n", cex.lab = 0.7, lwd = 1.5, cex = 0.7)
      x_ticks <- axis(1, at = round(te_first, digits = 4), cex.axis = 0.55, labels = FALSE, lwd = 0, lwd.ticks = 0.5, col.ticks = "lightgray")
      text_y <- par("usr")[3] - (max(yenv, na.rm = TRUE) - min(yenv, na.rm = TRUE))/10 
      text(x = x_ticks, y = text_y, srt = 330, xpd = NA, labels = x_ticks, cex = 0.55, adj = 0.1)
      y_ticks <- axis(2, at = round(yenv, 2), cex.axis = 0.55, las = 2, lwd = 0, lwd.ticks = 0.5, col.ticks = "lightgray")
      abline(v = x_ticks, lwd = 0.5, lty = 3)
      abline(h = y_ticks, lwd = 0.5, lty = 3, col = "lightgray")
      abline(h = y, lty = 1, col = "darkblue", lwd = 0.7)
      legend("bottomright", "y", cex = 0.7, col = "darkblue", xpd = TRUE, horiz = TRUE, bty = "n", inset = c(-0.03, 1), lwd = 1.5, lty = 1)
    } else {
  plot(te, yenv, type = "o", xaxt = "n", xlab = "time (sec)", ylab = "y (enveloped)",  yaxt = "n", bty = "n", cex.lab = 0.7, lwd = 1.5, cex = 0.7)
  # x_ticks <- axis(1, at = seq(minSecs, maxSecs, maxSecs/10), cex.axis = 0.55, labels = FALSE, lwd = 0, lwd.ticks = 0.5, col.ticks = "lightgray")
  x_ticks <- axis(1, at = seq(minSecs, maxSecs, ePeriod), cex.axis = 0.55, labels = FALSE, lwd = 0, lwd.ticks = 0.5, col.ticks = "lightgray")
  # text_y <- par("usr")[3] - abs(y-yhat)/10 # Substracting from y minimum one tenth of the range between y and yhat.
  text_y <- par("usr")[3] - (max(yenv, na.rm = TRUE) - min(yenv, na.rm = TRUE))/10 
  # text(x = seq(minSecs, maxSecs, maxSecs/10), y = text_y, srt = 330, xpd = NA, labels = seq(minSecs, maxSecs, maxSecs/10), cex = 0.55, adj = 0.1)
  text(x = x_ticks, y = text_y, srt = 330, xpd = NA, labels = x_ticks, cex = 0.55, adj = 0.1)
  # y_ticks <- axis(2, at = round(seq(min(y, yhat), max(y, yhat), length.out = 10), 2), cex.axis = 0.55, las = 2, lwd = 0, lwd.ticks = 0.5, col.ticks = "lightgray")
  y_ticks <- axis(2, at = round(yenv, 2), cex.axis = 0.55, las = 2, lwd = 0, lwd.ticks = 0.5, col.ticks = "lightgray")
  abline(v = x_ticks, lwd = 0.5, lty = 3)
  abline(h = y_ticks, lwd = 0.5, lty = 3, col = "lightgray")
  abline(h = yhat, lty = 1, col = "darkorange", lwd = 0.7)
  abline(h = y, lty = 1, col = "darkblue", lwd = 0.7)
  lines(te[-1], yenv_sm$y, col = "steelblue", lwd = 2)
  # abline(v = x_ticks, lty = "dotted", lwd = 0.5)
  legend("bottomright", if (yhat %in% yenv) c("y", "yhat") else "y", cex = 0.7, col = if (yhat %in% yenv) c("darkblue", "darkorange") else "darkblue", xpd = TRUE, horiz = TRUE, bty = "n", inset = c(-0.03, 1), lwd = 1.5, lty = 1)
  # text.width = c(15, 30)
    }
  } else {
   # return(tibble(timeSec = te_first, yenv = yenv_first)) # cos first value at time 0 is nan
   # return(tibble(timeSec = te[!is.nan(yenv)], yenv = yenv[!is.nan(yenv)])) # cos first value at time 0 is nan
    return(data.table::data.table(timeSec = te_first, yenv = yenv_first))
  }
}

# vectorized without plotting and computations taking place only between the first and second period of envelope signal:
response_vec <- function(y, yhat, id = NULL, nEnv = NA, retTime = FALSE) {
  # y : Vector of y response values.
  # yhat : Vector of yhat predicted values.
  # id : Vector of id values for each y, yhat pair. This serves as an identifier so that the result can be joined to other data containg the same id. If NULL then an integer id in sequence is returned. 
  # nEnv : The number of envelope y response values to return for each y, yhat pair. These are envelope values that are closest to the observed y value. If NA the default, then all values are returned which can lead to extreme increases in sample size due to very low errors i.e. yhat is very close to y. When this occurs the envelope values place larger weight to the y, yhat pairs that had the smallest error or difference. This might not be desired, and then by using a non NULL integer value for nEnv each y, yhat pair can have the same number of enveloped values returned. In this way the result might better represent smoothness in the data without the effect of having different weights for each y, yhat pair. nEnv can be treated as a tuning parameter. 
  # retTime : Logical. Should the used envelope times be returned in the data.table object? Defaults to FALSE since commonly the result will be joined to the original time data that provided the y and yhat values. 
  #
  # browser()
  if (any(y<0,yhat<0)) stop("---y and yhat values can't be negative---")
  if (any(y == yhat)) stop("---There's a case with equal y and yhat values. Zero error, really?---")
  if (is.null(id)) id <- 1:length(y)
  if (!(is.na(nEnv) | is.numeric(nEnv))) stop("---nEnv should either be NA or have an integer value. If numeric then it's rounded to the nearest integer---")
  n_y <- length(y)
  if (any(n_y != length(yhat), n_y != length(id))) stop("---y, yhat and id vector lengths are not the same---")
  ePeriod <- 1/abs(y-yhat)
  # length_out <- (maxSecs/ePeriod)*2*abs(y-yhat)
  halfDev <- abs(y - yhat)/2
  center <- apply(matrix(c(y, yhat), ncol = 2), MARGIN = 1, mean)
  max_freq <- center+halfDev
  # Focusing time to be between first and second period (for each y-yhat pair).
  
  # before:
  # te <- lapply(1:n_y, function(i) seq(minSecs[i], maxSecs[i], by = 1/(2*max_freq[i])))
  # better cos due to nEnv value, no need to compute on everything between first and second period:
  te <- lapply(1:n_y, function(i) {
    if (is.na(nEnv <- round(nEnv))) {
   te_seq <- seq(ePeriod[i], 2*ePeriod[i], by = 1/(2*max_freq[i])) 
    } else {
      # browser()
      # lower bound condition so that no sampling occurs below the end of the first period:
      if((lb <- 2*ePeriod[i] - nEnv*1/(2*max_freq[i])) < ePeriod[i]) lb <- ePeriod[i]
      te_seq <- rev(seq(2*ePeriod[i], lb, by = -1/(2*max_freq[i])))
    }
   te_seq
   # te_seq[-length(te_seq)] # actually last value is needed in order to include also the observed y response value together with the envelope ones
  })
  # in sec
  # no need, same as te above:
  # te_first <- lapply(1:n_y, function(i) te[[i]][te[[i]] >= ePeriod[i] & te[[i]] < 2*ePeriod[i]])
  
  e <- lapply(1:n_y, function(i) 2*cos(2*pi*halfDev[i]*te[[i]]) * cos(2*pi*center[i]*te[[i]]))

  residNew <- lapply(1:n_y, function(i) acos(e[[i]] / (2 * cos(2*pi*center[i]*te[[i]]))) / (pi*te[[i]]))

  # extracting yenv values between first and second period (for each y-yhat pair):
  yenv <- lapply(1:n_y, function(i) { 
    if (y[i]>yhat[i]){
      y[i] - residNew[[i]]
      # time series case:
      # rev(y[i] - residNew[[i]]) # underestimation means that the observed y should come first in the sequence result # actually no because based on the envelope definition the enveloped values converge to the observed value in the time between first and second period, hence the observed response should always come last. 
    } else {
      y[i] + residNew[[i]] 
      }
    })
  # Already have added the observed value y value together with the enveloped ones, so below line not needed.
  # yenv <- lapply(1:n_y, function(i) c(yenv[[i]], y[[i]]))

  ys <- lapply(1:n_y, function(i) rep(y[i], length(yenv[[i]])))
  yHats <- lapply(1:n_y, function(i) rep(yhat[i], length(yenv[[i]])))
  
  if (retTime) {
    datOut <- lapply(1:n_y, function(i) data.table::data.table(id = id[i], t = te[[i]], y = ys[[i]], yHat = yHats[[i]], yEnv = yenv[[i]]))
    #  # time series case: (initial thought but no)
    # datOut <- lapply(1:n_y, function(i) data.table::data.table(id = id[i], t = if (y[i]>yhat[i]) {rev(te[[i]])} else {te[[i]]}, y = ys[[i]], yHat = yHats[[i]], yEnv = yenv[[i]]))
  } else {
    datOut <- lapply(1:n_y, function(i) data.table::data.table(id = id[i], y = ys[[i]], yHat = yHats[[i]], yEnv = yenv[[i]]))
  }
  
  datOut <- do.call(rbind, datOut)
  return(datOut)
}
 
response_vec(c(180,180,170,160), c(140,160,160,140))
response_vec(c(180,180,170,160), c(140,160,160,140), retTime = TRUE)
response_vec(c(140,160,160,140), c(100,140,150,120)) # underestimation
response_vec(c(140,160,160,140), c(180,180,170,160)) # overestimation
response_vec(c(180,180,170,160), c(140,160,160,140), id = 39:42)
response_vec(c(180,180,170,160), c(140,160,160,140), id = 39:41)
response_vec(c(180,180,170,160), c(140,160,160,-140))
response_vec(c(180,180,170,160), c(140,160,160))
response_vec(c(180,180,170,160), c(140,160,160,140), nEnv = 5)
response_vec(c(180,180,170,160), c(140,160,160,140), nEnv = 20)
#
response(180, 140, plot = FALSE)
response(180, 160, plot = FALSE)
### Replaced code:
# Instead of this, the more simple substracting from y minimum one tenth of the range between y and yhat is used. 
# text_y <- ifelse(par("usr")[3] > 100, par("usr")[3] - 5, par("usr")[3] - 1)
  # res_par <- as.character(max(which(sapply(c(0,20,50,1e2,1e4), function(x){par("usr")[3] >= x}))))
  # use_par <- 0
  # text_y <- switch(res_par, 
  #        "1" = {use_par <- par("usr")[3]-1},
  #        "2" = {use_par <- par("usr")[3]-2},
  #        "3" = {use_par <- par("usr")[3]-5},
  #        "4" = {use_par <- par("usr")[3]-10},
  #        "5" = {use_par <- par("usr")[3]-20})
#
# if (is.na(te)) te <- seq(0, maxSecs, by = 1/(2*10*max_freq)) # in sec 
  
  # # if (length_out/1e3 < 1) {length_out <- length_out*100}
  # if (abs(y-yhat) >= 1) { # this to sample enough points without increasing the computational time
  #  pows <- 10^c(0,0,2,4,6,8,10,12)
  #  idx <- max(which(sapply(c(1,10,1e2,1e3,1e4,1e5,1e6,1e7), function(x) abs(y-yhat)%%x) / pows == 0))
  #  length_out <- length_out / pows[idx] 
  # }
  # if (abs(y-yhat) >= 100 & maxSecs > 1e2) {
  #   tEnd <- ePeriod*1e2
  #   message("Adjusted tEnd to ",  tEnd, " msec i.e. 100 times the envelope's period, since the envelope's period is only", paste0(" ", ePeriod), " Hz.")
  # }
  # 
  # to be checked for abs(y-yhat) < 1
###

response(180, 140)
response(180, 140, zoom_first = TRUE)
response(180, 140, plot = FALSE)
response(180, 220) # like mirror image
response(180, 160) # better prediction # higher frequency or longer time to reach observed value
response(180, 160, zoom_first = TRUE)
response(180, 160, plot = FALSE)
response(180, 200)
response(180, 140, eperiods = 50) # too many 
response(180, 100) # terrible prediction
response(180, 280) # terrible prediction
response(18, 14) # other scale, same as response(180, 140)
response(18, 22) # other scale
response(8, 4) # other scale
response(8, 10) # other scale
response(0.8, 0.6) # other scale 
response(1800, 1400) # other scale
response(18000, 14000) # checked
response(180000, 140000) # checked
response(1800000, 1400000) # checked
response(180, 175) # small error
# just the y values:
response(180, 140, plot = FALSE)

response(180, 140, plot = FALSE)
# A tibble: 9 × 2
response(180, 160, plot = FALSE)
# A tibble: 18 × 2
response(180, 170, plot = FALSE)
# A tibble: 36 × 2
response(180, 175, plot = FALSE)
# A tibble: 72 × 2
response(180, 179, plot = FALSE)
# # A tibble: 360 × 2
response(180, 179.9, plot = FALSE)
# A tibble: 3,600 × 2
```

# Not used

```{r}
# monthly frequency: 12 points in one year (one cycle)
?forecast::findfrequency
responseFreqs <- function(y, yhat, k = 15, plot = TRUE, verbose = TRUE) { 
  # browser()
  ePeriod <- 1e3/abs(y-yhat)
  res <- sapply(2:k, function(i){
  if (verbose) cat(paste0(i, "\n"))
  yenv <- response(y, yhat, eperiodStart = i, eperiods = 5*1e2, plot = FALSE)
  forecast::findfrequency(yenv)
  })
  te <- seq(2*ePeriod, k*ePeriod, ePeriod)
  if (plot) plot(te, res) 
  teFreq1 <- te[which(res == 1)]
  teFreq1Idx <- which(round(diff(teFreq1), 2) == round(ePeriod, 2))[1]
  list(ePeriod = ePeriod, freqMode = as.integer(names(which.max(table(res)))), freqMean = as.integer(mean(res)), mode1Secs = teFreq1[teFreq1Idx])
}
#
responseFreqs(180, 120, k = 30) # NA, a signal for bad prediction?
responseFreqs(180, 130, k = 30)
responseFreqs(180, 140, k = 30)
responseFreqs(180, 150, k = 30)
responseFreqs(180, 160, k = 30)
responseFreqs(180, 170, k = 30)
responseFreqs(180, 175, k = 30)

mode1SecsEx <- sapply(seq(125, 175, 2), function(yhat) responseFreqs(180, yhat, k = 30, plot = FALSE, verbose = FALSE)$mode1Secs) # pre 125 you get NAs, check
plot(seq(125, 175, 2), mode1SecsEx)

responseFreqs(180, 140, eperiods = 1e3)
# ePeriod freqMode freqMean 
#       25       20       26 
responseFreqs(180, 150, eperiods = 1e3)
#  ePeriod freqMode freqMean 
# 33.33333 62.00000 53.00000 
responseFreqs(180, 160, eperiods = 1e3)
# ePeriod freqMode freqMean 
#       50       43       43
responseFreqs(180, 170, eperiods = 1e3)
# ePeriod freqMode freqMean 
#      100        1       40 
# 100 was reached for some time.
responseFreqs(180, 175, eperiods = 1e3) # takes longer
# ePeriod freqMode freqMean 
#      200        1        1
#
responseFreqs(180, 140)
# ePeriod freqMode freqMean 
#       25       20       21 
responseFreqs(180, 150)
#  ePeriod freqMode freqMean 
# 33.33333 31 30
responseFreqs(180, 160)
# ePeriod freqMode freqMean 
#       50       48       47
responseFreqs(180, 170)
# ePeriod freqMode freqMean 
#      100        2       8 
# 100 was reached for some time.
responseFreqs(180, 175)
# ePeriod freqMode freqMean 
#      200        2        4
     
response(180, 170, eperiods = 500)


responseFreqs <- sapply(1:30, function(i){
 cat(paste0(i, "\n"))
 yenv <- response(180, 140, tStart = i*(1e3/abs(180-140)), tEnd = 1e3, plot = FALSE)
 forecast::findfrequency(yenv) # 250 
})
median(responseFreqs[responseFreqs > 5]) # tEnd = 750 # 250
plot(seq(25, 750, 25), responseFreqs)
#

# How many cycles for convergence. 
responseConv <- function(y, yhat, cycleCorrection = 10, tEnd = 1e3, ePeriod = 1e3/abs(y-yhat), length_out = (tEnd/ePeriod)*2*abs(y-yhat)+1, plot = TRUE, freqThres = 5) {
  # ePeriod is in msecs
  # browser()
  k <- 1:((tEnd/ePeriod)-cycleCorrection)
  responseFreqs <- sapply(k, function(i){
  cat(paste0(i, "\n"))
  # Calculate the response for consecutive multiples of the ePeriod, starting from the first ePeriod completion. 
  # response(y, yhat, eperiodStart = i*ePeriod/1e3, eperiods = 30, plot = TRUE)
  yenv <- response(y, yhat, eperiodStart = i*ePeriod/1e3, eperiods = 30, plot = FALSE)
  # yenv <- response(y, yhat, eperiodStart = i*ePeriod, eperiods = 20*1e2, plot = FALSE)
  # yenv <- response(y, yhat, tStart = i*ePeriod, tEnd = tEnd, plot = FALSE)
  forecast::findfrequency(yenv) 
  })
  if (plot) plot(seq(ePeriod, max(k)*ePeriod, ePeriod), responseFreqs) # from tStart to tEnd
  res <- list(medianFreq = stats::median(responseFreqs), 
           modeFreq = as.numeric(names(sort(-table(responseFreqs)))[1]), 
           medianFreqThres = stats::median(responseFreqs[responseFreqs > freqThres]), 
           maxStartTime = max(k)*ePeriod, 
           resData = tibble(responseFreqs = responseFreqs, responseTimes = k*ePeriod))
  return(res)
}

envelope(y = 180, yhat = 140, both = T) # underestimation
# $resData
#   samples halfDev center ePeriodSecs eFreq
# 1     361      20    160       0.025    40

responseConv(180, 140, cycleCorrection = 0)
responseConv(180, 140, cycleCorrection = 0, tEnd = 200)
unique(response(180, 140, tStart = 1e3, tEnd = 1e3, plot = FALSE))
# [1] 180
responseFreqsEx <- responseConv(180, 140, tEnd = 1700)
responseFreqsEx
responseConv(180, 140, tEnd = 1700) # converged
responseConv(180, 140, tEnd = 1450)

responseConv(180, 140, tEnd = 2e3)
responseConv(180, 140, tEnd = 3e3, cycleCorrection = 50)
responseConv(180, 140, tEnd = 5e3, cycleCorrection = 50)
responseConv(180, 140, tEnd = 10e3, cycleCorrection = 200)

1/18
# [1] 0.05555556
response(180, 140)

responseConv(180, 160, tEnd = 10e3, cycleCorrection = 100)
1/36
response(180, 160)
envelope(y = 180, yhat = 160, both = T) 
```

# Repeated Enveloped Response Application For Multiple Residuals 

## Data 1 Example - Import

Using case and RStudio environement at 
/Users/aristidiskoitsanos/Desktop/ari/pnoe/respRateAtExercice_SlidingWindow

```{r}
load("/Users/aristidiskoitsanos/Desktop/ari/pnoe/respRateAtExercice_SlidingWindow/datFeatProcessed_V6.RData")
# Keeping only the brDat_sw60_by60_hr object on heart features in the workspace.
library(tidyverse)
testEx <- brDat_sw60_by60_hr |> distinct(dataID) |> slice_sample(n = 1) |> pull(dataID)

brDat_sw60_by60_hr |> filter(dataID == testEx) |> select(dataID, endSec_60, hr_H1_60) |> 
  rename(t = endSec_60, hr = hr_H1_60) |> 
  with({
    # browser()
    cat(unique(dataID))
    plot(t, hr, type = "p", ylim = c(min(hr), max(hr)), xlab = "time (sec)", ylab = "hr", xaxt = "n", yaxt = "n")
  x_ticks <- axis(1, at = seq(min(t), max(t), by = round(range(t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(hr), max(hr), by = round(range(hr)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  abline(v = x_ticks, h = y_ticks, lwd = 0.7, lty = 3, col = "lightgray")
  # points(sampleSize[mod == "lm"], mseInt[mod == "lm"], col = scales::alpha("firebrick1", 0.5), bg = scales::alpha("firebrick1", 0.5), pch = 21)
  # points(sampleSize[mod == "nls"], mseInt[mod == "nls"], col = scales::alpha("darkgreen", 0.5), bg = scales::alpha("darkgreen", 0.5), pch = 21)
  # legend("bottomright", c("lm (misspecified)", "nls (correct)"), cex = 1, col = c("firebrick1", "darkgreen"), pch = c(16,16), inset = c(0.03, 0.95), xpd = TRUE, horiz = TRUE, bty = "n", text.width = c(60, 30))
  # smFit <- stats::smooth.spline(sampleSize[mod == "lm"], mseInt[mod == "lm"], df = 3)
  # lines(smFit$x, smFit$y, col = "firebrick1", lwd = 3)
  # smFit <- stats::smooth.spline(sampleSize[mod == "nls"], mseInt[mod == "nls"], df = 3)
  # lines(smFit$x, smFit$y, col = "darkgreen", lwd = 3)
  })

# bf vs hr max in 60 sec sliding window
# 16262269-3068-4798-34FB-EED9F70AD908
# B3C413DC-3CF7-4855-1566-AA0A15DFD808
# 02ECB6B5-7357-4E0F-D250-829D0A6AD908
# 60A2D64E-0492-4250-D34D-AB4AD930D908
# 58B20CC7-486A-4B65-BBC4-964C424CD908
# 512FE78E-90A1-465F-2E2C-5A294466D908
# 450E2E27-E5CF-4C16-AA6B-AD64441ED908
# 8735F696-839D-499E-8EBF-888D9BF2D808

# time vs hr max in 60 sec sliding window
# 3C4FF01D-33BA-4611-70FE-9E1EB7F9D808
# D7C10230-FBCE-44B4-A5D9-942DA47ED908
# F728B6F3-4440-4EDF-A476-83966D26D908
# B49B161E-9495-46C3-5061-B9FA7B7CD908 # different pattern
# 14EE38A4-3B84-4B61-E2EE-B58C1ED7D808
# 8F201C89-9CB9-4C6E-ADC6-64920B21D908 # different pattern
# DF73D317-C699-448D-E06A-253A2D53D908
# 8B45E751-BDAC-48C0-94D3-DFE07BD9D808 # step function
# 0C36E651-73CB-4455-0049-4D64027DD908 # different pattern i.e. on-off and repeat
# 02F13E1F-27E1-4021-2BDE-42EEC435D908 # all over the place
# The above are treadmill type of exercises of Ramp type (warmup, ramp, recovery) with exception the step function which is HR hlaupatest endurance.

readBreaths <- function(flnm) {
  read_delim(flnm, delim = ",", escape_double = FALSE, trim_ws = TRUE) %>% 
    select(PHASE) %>% 
    rename(protocol_phase = PHASE) %>% 
    mutate(dataID = gsub(".*[/]([^.]+)[_].*", "\\1", flnm))
}

breathsExerData210928 <- list.files(path = "/Users/aristidiskoitsanos/Desktop/ari/pnoe/dataSets/data_exercise_pool_3", pattern = "*_breaths.csv", full.names = TRUE) %>% 
    map_df(~ readBreaths(.)) 

breathsExerData210928 <- breathsExerData210928 |> na.omit() |> distinct()

unique(breathsExerData210928$protocol_phase)
# remove Warm Up, -, Ramp, Recovery, PNOE, PNOĒ, Test, (, )
toRemove <- c("Warm Up", "-", "Ramp", "Recovery", "PNOE", "PNOĒ", "Test", "/", "warm-up", "(", ")", "WarmUp", "Exhaustion", "Warm-up", "RAMP", "ramp", "warmup")

protocols <- breathsExerData210928 |> 
  mutate(protocol_phase = str_remove_all(string = protocol_phase, 
                                         pattern = paste(toRemove, collapse = "|")))

unique(protocols$protocol_phase) # need to remove white space

protocols <- protocols |> mutate(protocol_phase = str_remove_all(string = protocol_phase, 
                                         pattern = paste(" ", collapse = "|")))
unique(protocols$protocol_phase)
sort(table(protocols$protocol_phase)) 
# keep the most popular:
protocols <- names(sort(table(protocols$protocol_phase), decreasing = TRUE)[1:3])
# "Treadmill" "Bike"      "Flywheel"

hrDat_sw60_by60 <- brDat_sw60_by60_hr |> select(dataID, endSec_60, hr_H1_60) |> 
  inner_join(breathsExerData210928 |> na.omit() |> 
               filter(str_detect(protocol_phase, paste(protocols, collapse = "|"))) |> 
               mutate(protocol_phase = str_extract(string = protocol_phase, pattern = paste(protocols, collapse = "|"))) |> 
               distinct(dataID, protocol_phase), 
             by = "dataID", relationship = "many-to-many") |> 
  rename(exercise_type = protocol_phase)

proportions(xtabs(~ exercise_type, hrDat_sw60_by60 |> select(exercise_type)))
# exercise_type
#      Bike  Flywheel Treadmill 
# 0.3303874 0.1021002 0.5675125 

# tests per exercise type:
hrDat_sw60_by60 |> select(exercise_type, dataID) |> distinct() |> 
  count(exercise_type, name = "tests")

# Treadmill:
hrDat_sw60_by60 |> filter(exercise_type == protocols[1]) |>  
  rename(t = endSec_60, hr = hr_H1_60) |> 
  with({
    # browser()
    # cat(unique(dataID))
    plot(t, hr, type = "p", ylim = c(min(hr), max(hr)), xlab = "time (sec)", ylab = "hr", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.008), bg = scales::alpha("steelblue", 0.008), pch = 21)
  x_ticks <- axis(1, at = seq(min(t), max(t), by = round(range(t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(hr), max(hr), by = round(range(hr)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  abline(v = x_ticks, h = y_ticks, lwd = 0.7, lty = 3, col = "lightgray")
  # points(sampleSize[mod == "lm"], mseInt[mod == "lm"], col = scales::alpha("firebrick1", 0.5), bg = scales::alpha("firebrick1", 0.5), pch = 21)
  # points(sampleSize[mod == "nls"], mseInt[mod == "nls"], col = scales::alpha("darkgreen", 0.5), bg = scales::alpha("darkgreen", 0.5), pch = 21)
  # legend("bottomright", c("lm (misspecified)", "nls (correct)"), cex = 1, col = c("firebrick1", "darkgreen"), pch = c(16,16), inset = c(0.03, 0.95), xpd = TRUE, horiz = TRUE, bty = "n", text.width = c(60, 30))
  smFit <- stats::smooth.spline(x = t, y = hr, cv = FALSE)
  lines(smFit$x, smFit$y, col = "firebrick1", lwd = 3)
  # smFit <- stats::smooth.spline(sampleSize[mod == "nls"], mseInt[mod == "nls"], df = 3)
  # lines(smFit$x, smFit$y, col = "darkgreen", lwd = 3)
  title(unique(exercise_type))
  })
# Specific protocol:
protocols_spec_tread <- c("Warm Up - Treadmill Ramp Test - PNOĒ", "Ramp - Treadmill Ramp Test - PNOĒ", "Recovery - Treadmill Ramp Test - PNOĒ")

hrDat_sw60_by60 |> 
  inner_join(breathsExerData210928 |> filter(protocol_phase %in% protocols_spec_tread) |> 
               group_by(dataID) |> 
               filter(n() == 3) |> # check all id have 3: ungroup() |> count(protocol_phase)
               distinct(dataID), 
             by = "dataID") |> # check only Treadmill: |> count(exercise_type)
  rename(t = endSec_60, hr = hr_H1_60) |> 
  group_by(dataID) |> 
  filter(last(t) >= 840, last(t) <= 960, max(hr) <= 240) |> # last(t) <= 900 # last(t) <= 1100
  ungroup() |> # tests that lasted between 14 and 16 minutes
  with({
    plot(t, hr, type = "p", ylim = c(min(hr), max(hr)), xlab = "time (sec)", ylab = "hr", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.05), bg = scales::alpha("steelblue", 0.05), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(t), max(t), by = round(range(t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(hr), max(hr), by = round(range(hr)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  abline(v = x_ticks, h = y_ticks, lwd = 0.7, lty = 3, col = "lightgray")
  smFit <- stats::smooth.spline(x = t, y = hr, cv = FALSE) # GCV
  lines(smFit$x, smFit$y, col = "firebrick1", lwd = 3)
  title(unique(exercise_type))
  })

# Bike:

# check on protocol:
hrDat_sw60_by60 |> filter(exercise_type == "Bike") |> 
  inner_join(breathsExerData210928 , 
             by = "dataID") |> count(protocol_phase, sort = T)
# Specific protocol:
protocols_spec_bike <- c("Warm Up - Bike Ramp Test - PNOĒ", "Ramp - Bike Ramp Test - PNOĒ", "Recovery - Bike Ramp Test - PNOĒ")

hrDat_sw60_by60 |> 
  inner_join(breathsExerData210928 |> filter(protocol_phase %in% protocols_spec_bike) |> 
               group_by(dataID) |> 
               filter(n() == 3) |> # check all id have 3: ungroup() |> count(protocol_phase)
               distinct(dataID), 
             by = "dataID") |> # check only Bike: count(exercise_type)
  rename(t = endSec_60, hr = hr_H1_60) |> 
  group_by(dataID) |> 
  filter(last(t) >= 840, last(t) <= 960, max(hr) <= 240) |> # last(t) <= 900 # last(t) <= 1100
  ungroup() |> # tests that lasted between 14 and 16 minutes
  with({
    plot(t, hr, type = "p", ylim = c(min(hr), max(hr)), xlab = "time (sec)", ylab = "hr", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.05), bg = scales::alpha("steelblue", 0.05), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(t), max(t), by = round(range(t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(hr), max(hr), by = round(range(hr)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  abline(v = x_ticks, h = y_ticks, lwd = 0.7, lty = 3, col = "lightgray")
  smFit <- stats::smooth.spline(x = t, y = hr, cv = FALSE) # GCV
  lines(smFit$x, smFit$y, col = "firebrick1", lwd = 3)
  title(unique(exercise_type))
  })

# Flywheel:

# check on protocol:
hrDat_sw60_by60 |> filter(exercise_type == "Flywheel") |> 
  inner_join(breathsExerData210928 , 
             by = "dataID") |> count(protocol_phase, sort = T)
# Specific protocol:
protocols_spec_fly <- c("Warm Up - Flywheel Ramp Test - PNOĒ", "Flywheel Test", "Recovery - Flywheel Ramp Test - PNOĒ")

hrDat_sw60_by60 |> 
  inner_join(breathsExerData210928 |> filter(protocol_phase %in% protocols_spec_fly) |> 
               group_by(dataID) |> 
               filter(n() == 3) |> # check all id have 3: ungroup() |> count(protocol_phase)
               distinct(dataID), 
             by = "dataID") |> # check only Flywheel: count(exercise_type)
  rename(t = endSec_60, hr = hr_H1_60) |> 
  group_by(dataID) |> 
  filter(last(t) >= 840, last(t) <= 960, max(hr) <= 240) |> # last(t) <= 900 # last(t) <= 1100
  ungroup() |> # tests that lasted between 14 and 16 minutes
  with({
    plot(t, hr, type = "p", ylim = c(min(hr), max(hr)), xlab = "time (sec)", ylab = "hr", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.05), bg = scales::alpha("steelblue", 0.05), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(t), max(t), by = round(range(t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(hr), max(hr), by = round(range(hr)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  abline(v = x_ticks, h = y_ticks, lwd = 0.7, lty = 3, col = "lightgray")
  smFit <- stats::smooth.spline(x = t, y = hr, cv = FALSE) # GCV
  lines(smFit$x, smFit$y, col = "firebrick1", lwd = 3)
  title(unique(exercise_type))
  })

# No visible diffs among treadmill, bike and flywheel. 

hrMaxBy60Dat <- hrDat_sw60_by60 |> 
  inner_join(breathsExerData210928 |> 
               filter(protocol_phase %in% c(protocols_spec_tread, protocols_spec_bike, protocols_spec_fly)) |> 
               group_by(dataID) |> 
               filter(n() == 3) |> # ungroup() |> count(protocol_phase)
               distinct(dataID), 
             by = "dataID") |> # count(exercise_type)
  rename(t = endSec_60, hr = hr_H1_60) |> 
  group_by(dataID) |> 
  filter(last(t) >= 840, last(t) <= 960, max(hr) <= 240) |> 
  ungroup()

hrMaxBy60Dat |> group_by(dataID) |> summarise(chck = last(t)) |> skimr::skim()
hrMaxBy60Dat |> group_by(dataID) |> summarise(chck = n()) |> skimr::skim()
hrMaxBy60Dat |> group_by(exercise_type) |> summarise(chck = n()) 

# acf:
testEx <- hrMaxBy60Dat |> distinct(dataID) |> slice_sample(n = 1) |> pull(dataID)

hrMaxBy60Dat |> filter(dataID == testEx) |> pull(hr) |> forecast::Acf()
hrMaxBy60Dat |> filter(dataID == testEx) |> 
  mutate(hrDiff = hr-lag(hr)) |> 
  na.omit() |> 
  pull(hrDiff) |> 
  forecast::Acf()

hrMaxBy60Dat |> filter(dataID == testEx, t < 720) |> pull(hr) |> forecast::auto.arima() |> forecast::forecast(h=7) |> plot()
hrMaxBy60Dat |> filter(dataID == testEx, t < 780) |> pull(hr) |> lines()
# but does not predict well the recovery
```

### Selecting one dataID for the first envelope example

```{r}
hrMaxBy60Dat |> distinct(dataID, exercise_type)

envDatEx1 <- hrMaxBy60Dat |> filter(exercise_type == "Bike") |> # distinct(dataID) #190
  filter(dataID == unique(dataID)[16]) |>  # View() # 4,5,16,26,32
  rowid_to_column(var = "rowID")

envDatEx1 |> 
  with({
       plot(t, hr, type = "p", ylim = c(min(hr), max(hr)), xlab = "time (sec)", ylab = "hr", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.4), bg = scales::alpha("steelblue", 0.4), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(t), max(t), by = round(range(t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(hr), max(hr), by = round(range(hr)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  abline(v = x_ticks, h = y_ticks, lwd = 0.7, lty = 3, col = "lightgray")
  smFit <- stats::smooth.spline(x = t, y = hr, cv = FALSE) # GCV
  lines(smFit$x, smFit$y, col = "firebrick1", lwd = 3)
  title(unique(dataID))
  })
envDatEx1_1 <- envDatEx1 |> 
  filter(t > 300, t < 655)
envDatEx1_2 <- envDatEx1 |> 
  filter(t > 655, t < 780)
envDatEx1_3 <- envDatEx1 |> 
  filter(t > 780)

rulerPlotEx <- function(.data) {
  .data |> 
  with({
    plot(t, hr, type = "p", ylim = c(min(hr), max(hr)), xlab = "time (sec)", ylab = "hr", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.4), bg = scales::alpha("steelblue", 0.4), pch = 21) # alpha 0.008 for more data
    x_ticks <- axis(1, at = seq(min(t), max(t), by = round(range(t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
    y_ticks <- axis(2, at = seq(min(hr), max(hr), by = round(range(hr)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
    abline(v = x_ticks, h = y_ticks, lwd = 0.7, lty = 3, col = "lightgray")
    smFit <- stats::smooth.spline(x = t, y = hr, cv = FALSE) # GCV
    lines(smFit$x, smFit$y, col = "firebrick1", lwd = 3)
    modDatEx1_1 <- lm(hr ~ t, data = .data)
    # modDatEx1_2 <- lm(hr ~ poly(t, degree = 4), data = .data)
    lines(t, fitted(modDatEx1_1), col = "blue", lwd = 3)
    # lines(t, fitted(modDatEx1_2), col = "darkgreen", lwd = 3)
    title(unique(dataID))
  })
}

rulerPlotEx(envDatEx1_1)
rulerPlotEx(envDatEx1_2)
rulerPlotEx(envDatEx1_3)

# simple model:
modDatEx1_1 <- lm(hr ~ t, data = envDatEx1_1)
#
mod1yHat <- envDatEx1_1 |> 
  mutate(yHat = as.numeric(fitted(modDatEx1_1))) |> 
  rename(y = hr) |> 
  select(rowID, t, y, yHat)

tictoc::tic()
mod1yHat <- mod1yHat |> rowwise() |> 
  mutate(yEnv = list(response(y = y, yhat = yHat, plot = FALSE)$yenv))
tictoc::toc() # 60 sec
mod1yHat |> print(n=82) # a few big numbers due to very good fits
mod1yHat |> unnest(cols = yEnv)
# # A tibble: 88,439 × 4 from 82 rows
# Using data.table:
mod1yHat_dt <- envDatEx1_1 |> 
  mutate(yHat = as.numeric(fitted(modDatEx1_1))) |> 
  rename(y = hr) |> 
  select(rowID, t, y, yHat) |> 
  data.table::as.data.table()
library(data.table) # maskings
# https://arelbundock.com/posts/datatable_rowwise/ # see section A USEFUL ALTERNATIVE
# f_response <- function(y, yHat) list(yEnv = response(y = y, yhat = yHat, plot = FALSE)[,'yenv'])
f_response <- function(y, yHat) list(yEnv = response(y = y, yhat = yHat, plot = FALSE)$yenv)
tictoc::tic()
mod1yHat_dt[, yEnv := Map(f_response, y, yHat)] # rowwise operation # yEnv is a list column
tictoc::toc()
# 58 sec # same as with tibble
# unnesting the yEnv list column:
mod1yHat_dt <- mod1yHat_dt[, yEnv[[1]], by = list(rowID, t, y, yHat)]
# 
# rerunning the mod1yHat_dt object above to start from the original data and then using the vectorized version of response function response_vec (also using data.table even though this does not seem to make a difference compared to using tibble):
mod1yHat_dt2 <- envDatEx1_1 |> 
  mutate(yHat = as.numeric(fitted(modDatEx1_1))) |> 
  rename(y = hr, id = rowID) |> 
  select(id, t, y, yHat) |> 
  data.table::as.data.table() # t does not to be present in general
tictoc::tic()
mod1yHat_dt2 <- response_vec(y = mod1yHat_dt2$y, yhat = mod1yHat_dt2$yHat, id = mod1yHat_dt2$id)
tictoc::toc()
####### 0.5 sec from 60 with rowwise operation before !!! #######
# joining the times i.e. for plotting:
mod1yHat_dt2[as.data.table(envDatEx1_1 |> rename(id = rowID))[,c("id","t")], on = .(id)]
# keeps same memory reference: 
# https://stackoverflow.com/questions/34598139/left-join-using-data-table
# https://medium.com/analytics-vidhya/r-data-table-joins-48f00b46ce29

# mod1yHat |> unnest(cols = yEnv) |> # tibble case
# mod1yHat_dt |>
mod1yHat_dt2[as.data.table(envDatEx1_1 |> rename(id = rowID))[,c("id","t")], on = .(id)] |>
  with({
    # browser()
       plot(t, yEnv, type = "p", ylim = c(min(yEnv), max(yEnv)), xlab = "time (sec)", ylab = "yEnv", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(t), max(t), by = round(range(t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(yEnv), max(yEnv), by = round(range(yEnv)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  # abline(v = x_ticks, h = y_ticks, lwd = 0.7, lty = 3, col = "lightgray") # maybe leave out
  points(t, y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  
  # quadratic:
  modDatEx1_y <- lm(y ~ poly(t, degree = 2), data = envDatEx1_1 |> rename(y = hr))
  modDatEx1_yEnv <- lm(yEnv ~ poly(t, degree = 2), data = data.table(t, yEnv))
  lines(envDatEx1_1$t, fitted(modDatEx1_y), col = "darkorange", lwd = 3)
  lines(t, fitted(modDatEx1_yEnv), col = "darkgreen", lwd = 3)
  # cubic:
  modDatEx1_y <- lm(y ~ poly(t, degree = 3), data = envDatEx1_1 |> rename(y = hr))
  modDatEx1_yEnv <- lm(yEnv ~ poly(t, degree = 3), data = data.table(t, yEnv))
  lines(envDatEx1_1$t, fitted(modDatEx1_y), col = "darkorange", lwd = 3)
  lines(t, fitted(modDatEx1_yEnv), col = "darkgreen", lwd = 3)
  # quartic:
  modDatEx1_y <- lm(y ~ poly(t, degree = 4), data = envDatEx1_1 |> rename(y = hr))
  modDatEx1_yEnv <- lm(yEnv ~ poly(t, degree = 4), data = data.table(t, yEnv))
  lines(envDatEx1_1$t, fitted(modDatEx1_y), col = "darkorange", lwd = 3)
  lines(t, fitted(modDatEx1_yEnv), col = "darkgreen", lwd = 3)
  # seventh degree:
  modDatEx1_y <- lm(y ~ poly(t, degree = 7), data = envDatEx1_1 |> rename(y = hr))
  modDatEx1_yEnv <- lm(yEnv ~ poly(t, degree = 7), data = data.table(t, yEnv))
  lines(envDatEx1_1$t, fitted(modDatEx1_y), col = "darkorange", lwd = 3)
  lines(t, fitted(modDatEx1_yEnv), col = "darkgreen", lwd = 3)
  # smoothing splines:
  smFit <- stats::smooth.spline(x = envDatEx1_1$t, y = envDatEx1_1$hr, cv = FALSE) # GCV
  lines(smFit$x, smFit$y, col = "darkorange", lwd = 3)
  smFit2 <- stats::smooth.spline(x = t, y = yEnv, cv = FALSE, tol = 0.00001) # GCV
  lines(smFit2$x, smFit2$y, col = "darkgreen", lwd = 3)
  })

#### procedure ####
# 1) Fit simple model.
# 2) Derive the enveloped data.
# 3) Use the enveloped data with a more complex model.
# 4) Compare the out-of-sample prediction error between the model in 3) and the same complexity model on the original (non-enveloped) data that were used in 1).

mod1yHat_dt2[as.data.table(envDatEx1_1 |> rename(id = rowID))[,c("id","t")], on = .(id)] |> 
  with({
    # browser()
  # simple linear:
  # modDatEx1_1 was run above and contains the linear model upon which the enveloped y values are derived
  # y enveloped is accessed in this environment
  # out-of-sample dataset:
  out_data <- envDatEx1_2 |> rename(y = hr)
  
  ### more complex:
  # quadratic:
  modDatEx1_y_quad <- lm(y ~ poly(t, degree = 2), data = envDatEx1_1 |> rename(y = hr))
  modDatEx1_yEnv_quad <- lm(yEnv ~ poly(t, degree = 2), data = data.table(t, yEnv))
  # out-of-sample predictions:
  modDatEx1_y_quad_preds <- predict(modDatEx1_y_quad, newdata = out_data)
  modDatEx1_yEnv_quad_preds <- predict(modDatEx1_yEnv_quad, newdata = out_data)
  
  plot(out_data$t, out_data$y, type = "p", ylim = c(min(out_data$y), max(out_data$y)), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(out_data$t), max(out_data$t), by = round(range(out_data$t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(out_data$y), max(out_data$y), by = round(range(out_data$y)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  # prediction lines:
  lines(out_data$t, modDatEx1_y_quad_preds, col = "darkorange", lwd = 3)
  lines(out_data$t, modDatEx1_yEnv_quad_preds, col = "darkgreen", lwd = 3)
  # yEnv based model is much better
  # overall visual:
  plot(c(t,out_data$t), c(y,out_data$y), type = "p", ylim = c(min(c(y,out_data$y)), max(c(y,out_data$y))), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(c(t,out_data$t)), max(c(t,out_data$t)), by = round(range(c(t,out_data$t))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(y,out_data$y)), max(c(y,out_data$y)), by = round(range(c(y,out_data$y))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  lines(c(envDatEx1_1$t, out_data$t), c(fitted(modDatEx1_y_quad), modDatEx1_y_quad_preds), col = "darkorange", lwd = 3)
  lines(c(t,out_data$t), c(fitted(modDatEx1_yEnv_quad), modDatEx1_yEnv_quad_preds), col = "darkgreen", lwd = 3)
  
  # cubic:
  modDatEx1_y_cubic <- lm(y ~ poly(t, degree = 3), data = envDatEx1_1 |> rename(y = hr))
  modDatEx1_yEnv_cubic <- lm(yEnv ~ poly(t, degree = 3), data = data.table(t, yEnv))
  # out-of-sample predictions:
  modDatEx1_y_cubic_preds <- predict(modDatEx1_y_cubic, newdata = out_data)
  modDatEx1_yEnv_cubic_preds <- predict(modDatEx1_yEnv_cubic, newdata = out_data)
  
  plot(out_data$t, out_data$y, type = "p", ylim = c(min(out_data$y), max(out_data$y)), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(out_data$t), max(out_data$t), by = round(range(out_data$t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(out_data$y), max(out_data$y), by = round(range(out_data$y)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  # prediction lines:
  lines(out_data$t, modDatEx1_y_cubic_preds, col = "darkorange", lwd = 3)
  lines(out_data$t, modDatEx1_yEnv_cubic_preds, col = "darkgreen", lwd = 3)
  # worse than quadratic model but yEnv based still visible
  # overall visual:
  plot(c(t,out_data$t), c(y,out_data$y), type = "p", ylim = c(min(c(y,out_data$y)), max(c(y,out_data$y))), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(c(t,out_data$t)), max(c(t,out_data$t)), by = round(range(c(t,out_data$t))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(y,out_data$y)), max(c(y,out_data$y)), by = round(range(c(y,out_data$y))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  lines(c(envDatEx1_1$t, out_data$t), c(fitted(modDatEx1_y_cubic), modDatEx1_y_cubic_preds), col = "darkorange", lwd = 3)
  lines(c(t,out_data$t), c(fitted(modDatEx1_yEnv_cubic), modDatEx1_yEnv_cubic_preds), col = "darkgreen", lwd = 3)
  
  # quartic:
  modDatEx1_y_quartic <- lm(y ~ poly(t, degree = 4), data = envDatEx1_1 |> rename(y = hr))
  modDatEx1_yEnv_quartic <- lm(yEnv ~ poly(t, degree = 4), data = data.table(t, yEnv))
  # out-of-sample predictions:
  modDatEx1_y_quartic_preds <- predict(modDatEx1_y_quartic, newdata = out_data)
  modDatEx1_yEnv_quartic_preds <- predict(modDatEx1_yEnv_quartic, newdata = out_data)
  
  plot(out_data$t, out_data$y, type = "p", ylim = c(min(out_data$y), max(out_data$y)), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(out_data$t), max(out_data$t), by = round(range(out_data$t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(out_data$y), max(out_data$y), by = round(range(out_data$y)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  # prediction lines:
  lines(out_data$t, modDatEx1_y_quartic_preds, col = "darkorange", lwd = 3)
  lines(out_data$t, modDatEx1_yEnv_quartic_preds, col = "darkgreen", lwd = 3)
  # worse than cubic model but yEnv based still visible
  
  # seventh degree:
  modDatEx1_y_seventh <- lm(y ~ poly(t, degree = 7), data = envDatEx1_1 |> rename(y = hr))
  modDatEx1_yEnv_seventh <- lm(yEnv ~ poly(t, degree = 7), data = data.table(t, yEnv))
  # out-of-sample predictions:
  modDatEx1_y_seventh_preds <- predict(modDatEx1_y_seventh, newdata = out_data)
  modDatEx1_yEnv_seventh_preds <- predict(modDatEx1_yEnv_seventh, newdata = out_data)
  
  plot(out_data$t, out_data$y, type = "p", ylim = c(min(out_data$y), max(out_data$y)), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(out_data$t), max(out_data$t), by = round(range(out_data$t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(out_data$y), max(out_data$y), by = round(range(out_data$y)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  # prediction lines:
  lines(out_data$t, modDatEx1_y_seventh_preds, col = "darkorange", lwd = 3)
  lines(out_data$t, modDatEx1_yEnv_seventh_preds, col = "darkgreen", lwd = 3)
  # worse than quadratic model but yEnv based still better
  # overall visual:
  plot(c(t,out_data$t), c(y,out_data$y), type = "p", ylim = c(min(c(y,out_data$y)), max(c(y,out_data$y))), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(c(t,out_data$t)), max(c(t,out_data$t)), by = round(range(c(t,out_data$t))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(y,out_data$y)), max(c(y,out_data$y)), by = round(range(c(y,out_data$y))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  lines(c(envDatEx1_1$t, out_data$t), c(fitted(modDatEx1_y_seventh), modDatEx1_y_seventh_preds), col = "darkorange", lwd = 3)
  lines(c(t,out_data$t), c(fitted(modDatEx1_yEnv_seventh), modDatEx1_yEnv_seventh_preds), col = "darkgreen", lwd = 3)
  
  # smoothing splines:
  smFit <- stats::smooth.spline(x = envDatEx1_1$t, y = envDatEx1_1$hr, cv = FALSE) # GCV
  smFit2 <- stats::smooth.spline(x = t, y = yEnv, cv = FALSE, tol = 0.00001) # GCV
  # out-of-sample predictions:
  modDatEx1_y_smspl_preds <- predict(smFit, x = out_data$t)$y
  modDatEx1_yEnv_smspl_preds <- predict(smFit2, x = out_data$t)$y
  
  plot(out_data$t, out_data$y, type = "p", ylim = c(min(out_data$y), max(out_data$y)), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(out_data$t), max(out_data$t), by = round(range(out_data$t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(out_data$y), max(out_data$y), by = round(range(out_data$y)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  # prediction lines:
  lines(out_data$t, modDatEx1_y_smspl_preds, col = "darkorange", lwd = 3)
  lines(out_data$t, modDatEx1_yEnv_smspl_preds, col = "darkgreen", lwd = 3)
  # worse than quadratic model but yEnv based still better
  # overall visual:
  plot(c(t,out_data$t), c(y,out_data$y), type = "p", ylim = c(min(c(y,out_data$y)), max(c(y,out_data$y))), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(c(t,out_data$t)), max(c(t,out_data$t)), by = round(range(c(t,out_data$t))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(y,out_data$y)), max(c(y,out_data$y)), by = round(range(c(y,out_data$y))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  lines(c(smFit$x,out_data$t), c(smFit$y, modDatEx1_y_smspl_preds), col = "darkorange", lwd = 3)
  lines(c(smFit2$x,out_data$t), c(smFit2$y, modDatEx1_yEnv_smspl_preds), col = "darkgreen", lwd = 3)
  
  # linear model for reference:
  modDatEx1_y_lin <- lm(y ~ t, data = envDatEx1_1 |> rename(y = hr))
  modDatEx1_yEnv_lin <- lm(yEnv ~ t, data = data.table(t, yEnv))
  # out-of-sample predictions:
  modDatEx1_y_lin_preds <- predict(modDatEx1_y_lin, newdata = out_data)
  modDatEx1_yEnv_lin_preds <- predict(modDatEx1_yEnv_lin, newdata = out_data)
  
  plot(out_data$t, out_data$y, type = "p", ylim = c(min(out_data$y), max(out_data$y)), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(out_data$t), max(out_data$t), by = round(range(out_data$t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(out_data$y), max(out_data$y), by = round(range(out_data$y)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  # prediction lines:
  lines(out_data$t, modDatEx1_y_lin_preds, col = "darkorange", lwd = 3)
  lines(out_data$t, modDatEx1_yEnv_lin_preds, col = "darkgreen", lwd = 3)
  # identical
  # overall visual:
  plot(c(t,out_data$t), c(y,out_data$y), type = "p", ylim = c(min(c(y,out_data$y)), max(c(y,out_data$y))), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(c(t,out_data$t)), max(c(t,out_data$t)), by = round(range(c(t,out_data$t))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(y,out_data$y)), max(c(y,out_data$y)), by = round(range(c(y,out_data$y))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  lines(c(envDatEx1_1$t, out_data$t), c(fitted(modDatEx1_y_lin), modDatEx1_y_lin_preds), col = "darkorange", lwd = 3)
  lines(c(t,out_data$t), c(fitted(modDatEx1_yEnv_lin), modDatEx1_yEnv_lin_preds), col = "darkgreen", lwd = 3)
  # identical result
  # For this data and in absence of other information such as an indicator for exercise intensity that would capture the heart rate jumps, a linear model seems the best (with the qaudratic being second best).
  # For the envelope procedure to make sense we need to capitalize on the residuals from some model. If the model on the enveloped ys is the same then we don't capitalize on prediction error and the result is the same as using the original data. However not yet sure if this is also the case in higher dimensions. 
  })

```

```{r}
envDatEx <- function(n, type = "Bike") {
  hrMaxBy60Dat |> filter(exercise_type == type) |> 
  filter(dataID == unique(dataID)[n]) |>  
  rowid_to_column(var = "rowID")
} 
# hrMaxBy60Dat |> count(exercise_type, dataID) |> count(exercise_type)
envDat <- envDatEx(n = sample(1:65, size = 1), type = sample(unique(hrMaxBy60Dat$exercise_type), size = 1))

envDat_1 <- envDat |> 
  filter(t > quantile(envDat$t)[2], t < quantile(envDat$t)[3]+100)
envDat_2 <- envDat |> 
  filter(t > quantile(envDat$t)[3]+100, t < quantile(envDat$t)[4])

modDat1 <- lm(hr ~ t, data = envDat_1)
#
modDat1_dt <- envDat_1 |> 
  mutate(yHat = as.numeric(fitted(modDat1))) |> 
  rename(y = hr, id = rowID) |> 
  select(id, t, y, yHat) |> 
  data.table::as.data.table()

modDat1_dt2 <- response_vec(y = modDat1_dt$y, yhat = modDat1_dt$yHat, id = modDat1_dt$id)

dev.off()

modDat1_dt2[as.data.table(envDat_1 |> rename(id = rowID))[,c("id","t")], on = .(id)] |> 
  with({
    # browser()
  # simple linear:
  # modDat1 was run above and contains the linear model upon which the enveloped y values are derived
  # y enveloped is accessed in this environment
  # out-of-sample dataset:
  out_data <- envDat_2 |> rename(y = hr)
  
  ### more complex:
  # quadratic:
  modDatEx1_y_quad <- lm(y ~ poly(t, degree = 2), data = envDat_1 |> rename(y = hr))
  modDatEx1_yEnv_quad <- lm(yEnv ~ poly(t, degree = 2), data = data.table(t, yEnv))
  # out-of-sample predictions:
  modDatEx1_y_quad_preds <- predict(modDatEx1_y_quad, newdata = out_data)
  modDatEx1_yEnv_quad_preds <- predict(modDatEx1_yEnv_quad, newdata = out_data)
  
  # # zoom on out of sample:
  # plot(out_data$t, out_data$y, type = "p", ylim = c(min(out_data$y), max(out_data$y)), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  # x_ticks <- axis(1, at = seq(min(out_data$t), max(out_data$t), by = round(range(out_data$t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  # y_ticks <- axis(2, at = seq(min(out_data$y), max(out_data$y), by = round(range(out_data$y)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  # points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  # # prediction lines:
  # lines(out_data$t, modDatEx1_y_quad_preds, col = "darkorange", lwd = 3)
  # lines(out_data$t, modDatEx1_yEnv_quad_preds, col = "darkgreen", lwd = 3)
  # overall visual:
  plot(c(t,out_data$t), c(y,out_data$y), type = "p", ylim = c(min(c(y,out_data$y)), max(c(y,out_data$y))), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(c(t,out_data$t)), max(c(t,out_data$t)), by = round(range(c(t,out_data$t))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(y,out_data$y)), max(c(y,out_data$y)), by = round(range(c(y,out_data$y))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  lines(c(envDat_1$t, out_data$t), c(fitted(modDatEx1_y_quad), modDatEx1_y_quad_preds), col = "darkorange", lwd = 3)
  lines(c(t,out_data$t), c(fitted(modDatEx1_yEnv_quad), modDatEx1_yEnv_quad_preds), col = "darkgreen", lwd = 3)
  title("poly 2")
  
  # cubic:
  modDatEx1_y_cubic <- lm(y ~ poly(t, degree = 3), data = envDat_1 |> rename(y = hr))
  modDatEx1_yEnv_cubic <- lm(yEnv ~ poly(t, degree = 3), data = data.table(t, yEnv))
  # out-of-sample predictions:
  modDatEx1_y_cubic_preds <- predict(modDatEx1_y_cubic, newdata = out_data)
  modDatEx1_yEnv_cubic_preds <- predict(modDatEx1_yEnv_cubic, newdata = out_data)
  
  # # zoom on out of sample:
  # plot(out_data$t, out_data$y, type = "p", ylim = c(min(out_data$y), max(out_data$y)), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  # x_ticks <- axis(1, at = seq(min(out_data$t), max(out_data$t), by = round(range(out_data$t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  # y_ticks <- axis(2, at = seq(min(out_data$y), max(out_data$y), by = round(range(out_data$y)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  # points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  # # prediction lines:
  # lines(out_data$t, modDatEx1_y_cubic_preds, col = "darkorange", lwd = 3)
  # lines(out_data$t, modDatEx1_yEnv_cubic_preds, col = "darkgreen", lwd = 3)
  # overall visual:
  plot(c(t,out_data$t), c(y,out_data$y), type = "p", ylim = c(min(c(y,out_data$y)), max(c(y,out_data$y))), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(c(t,out_data$t)), max(c(t,out_data$t)), by = round(range(c(t,out_data$t))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(y,out_data$y)), max(c(y,out_data$y)), by = round(range(c(y,out_data$y))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  lines(c(envDat_1$t, out_data$t), c(fitted(modDatEx1_y_cubic), modDatEx1_y_cubic_preds), col = "darkorange", lwd = 3)
  lines(c(t,out_data$t), c(fitted(modDatEx1_yEnv_cubic), modDatEx1_yEnv_cubic_preds), col = "darkgreen", lwd = 3)
  title("poly 3")
  
  # quartic:
  # modDatEx1_y_quartic <- lm(y ~ poly(t, degree = 4), data = envDat_1 |> rename(y = hr))
  # modDatEx1_yEnv_quartic <- lm(yEnv ~ poly(t, degree = 4), data = data.table(t, yEnv))
  # # out-of-sample predictions:
  # modDatEx1_y_quartic_preds <- predict(modDatEx1_y_quartic, newdata = out_data)
  # modDatEx1_yEnv_quartic_preds <- predict(modDatEx1_yEnv_quartic, newdata = out_data)
  
  # # zoom on out of sample:
  # plot(out_data$t, out_data$y, type = "p", ylim = c(min(out_data$y), max(out_data$y)), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  # x_ticks <- axis(1, at = seq(min(out_data$t), max(out_data$t), by = round(range(out_data$t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  # y_ticks <- axis(2, at = seq(min(out_data$y), max(out_data$y), by = round(range(out_data$y)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  # points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  # # prediction lines:
  # lines(out_data$t, modDatEx1_y_quartic_preds, col = "darkorange", lwd = 3)
  # lines(out_data$t, modDatEx1_yEnv_quartic_preds, col = "darkgreen", lwd = 3)
  
  # seventh degree:
  modDatEx1_y_seventh <- lm(y ~ poly(t, degree = 7), data = envDat_1 |> rename(y = hr))
  modDatEx1_yEnv_seventh <- lm(yEnv ~ poly(t, degree = 7), data = data.table(t, yEnv))
  # out-of-sample predictions:
  modDatEx1_y_seventh_preds <- predict(modDatEx1_y_seventh, newdata = out_data)
  modDatEx1_yEnv_seventh_preds <- predict(modDatEx1_yEnv_seventh, newdata = out_data)
  
  # # zoom on out of sample:
  # plot(out_data$t, out_data$y, type = "p", ylim = c(min(out_data$y), max(out_data$y)), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  # x_ticks <- axis(1, at = seq(min(out_data$t), max(out_data$t), by = round(range(out_data$t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  # y_ticks <- axis(2, at = seq(min(out_data$y), max(out_data$y), by = round(range(out_data$y)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  # points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  # # prediction lines:
  # lines(out_data$t, modDatEx1_y_seventh_preds, col = "darkorange", lwd = 3)
  # lines(out_data$t, modDatEx1_yEnv_seventh_preds, col = "darkgreen", lwd = 3)
  # overall visual:
  plot(c(t,out_data$t), c(y,out_data$y), type = "p", ylim = c(min(c(y,out_data$y)), max(c(y,out_data$y))), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(c(t,out_data$t)), max(c(t,out_data$t)), by = round(range(c(t,out_data$t))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(y,out_data$y)), max(c(y,out_data$y)), by = round(range(c(y,out_data$y))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  lines(c(envDat_1$t, out_data$t), c(fitted(modDatEx1_y_seventh), modDatEx1_y_seventh_preds), col = "darkorange", lwd = 3)
  lines(c(t,out_data$t), c(fitted(modDatEx1_yEnv_seventh), modDatEx1_yEnv_seventh_preds), col = "darkgreen", lwd = 3)
  title("poly 7")
  
  # smoothing splines:
  smFit <- stats::smooth.spline(x = envDat_1$t, y = envDat_1$hr, cv = FALSE) # GCV
  smFit2 <- stats::smooth.spline(x = t, y = yEnv, cv = FALSE, tol = 0.00001) # GCV
  # out-of-sample predictions:
  modDatEx1_y_smspl_preds <- predict(smFit, x = out_data$t)$y
  modDatEx1_yEnv_smspl_preds <- predict(smFit2, x = out_data$t)$y
  
  # # zoom on out of sample:
  # plot(out_data$t, out_data$y, type = "p", ylim = c(min(out_data$y), max(out_data$y)), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  # x_ticks <- axis(1, at = seq(min(out_data$t), max(out_data$t), by = round(range(out_data$t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  # y_ticks <- axis(2, at = seq(min(out_data$y), max(out_data$y), by = round(range(out_data$y)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  # points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  # # prediction lines:
  # lines(out_data$t, modDatEx1_y_smspl_preds, col = "darkorange", lwd = 3)
  # lines(out_data$t, modDatEx1_yEnv_smspl_preds, col = "darkgreen", lwd = 3)
  # overall visual:
  plot(c(t,out_data$t), c(y,out_data$y), type = "p", ylim = c(min(c(y,out_data$y)), max(c(y,out_data$y))), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(c(t,out_data$t)), max(c(t,out_data$t)), by = round(range(c(t,out_data$t))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(y,out_data$y)), max(c(y,out_data$y)), by = round(range(c(y,out_data$y))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  lines(c(smFit$x,out_data$t), c(smFit$y, modDatEx1_y_smspl_preds), col = "darkorange", lwd = 3)
  lines(c(smFit2$x,out_data$t), c(smFit2$y, modDatEx1_yEnv_smspl_preds), col = "darkgreen", lwd = 3)
  title("smoothing spline")
  
  # linear model for reference:
  modDatEx1_y_lin <- lm(y ~ t, data = envDat_1 |> rename(y = hr))
  modDatEx1_yEnv_lin <- lm(yEnv ~ t, data = data.table(t, yEnv))
  # out-of-sample predictions:
  modDatEx1_y_lin_preds <- predict(modDatEx1_y_lin, newdata = out_data)
  modDatEx1_yEnv_lin_preds <- predict(modDatEx1_yEnv_lin, newdata = out_data)
  
  # # zoom on out of sample:
  # plot(out_data$t, out_data$y, type = "p", ylim = c(min(out_data$y), max(out_data$y)), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  # x_ticks <- axis(1, at = seq(min(out_data$t), max(out_data$t), by = round(range(out_data$t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  # y_ticks <- axis(2, at = seq(min(out_data$y), max(out_data$y), by = round(range(out_data$y)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  # points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  # # prediction lines:
  # lines(out_data$t, modDatEx1_y_lin_preds, col = "darkorange", lwd = 3)
  # lines(out_data$t, modDatEx1_yEnv_lin_preds, col = "darkgreen", lwd = 3)
  # overall visual:
  plot(c(t,out_data$t), c(y,out_data$y), type = "p", ylim = c(min(c(y,out_data$y)), max(c(y,out_data$y))), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(c(t,out_data$t)), max(c(t,out_data$t)), by = round(range(c(t,out_data$t))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(y,out_data$y)), max(c(y,out_data$y)), by = round(range(c(y,out_data$y))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  lines(c(envDat_1$t, out_data$t), c(fitted(modDatEx1_y_lin), modDatEx1_y_lin_preds), col = "darkorange", lwd = 3)
  lines(c(t,out_data$t), c(fitted(modDatEx1_yEnv_lin), modDatEx1_yEnv_lin_preds), col = "darkgreen", lwd = 3)
  title("simple lm")
  })
```

### Reminder On Procedure

1) Fit simple model. Given a set of predictors, the simple model is linear regression with all predictors in the set (no interactions). If there are many predictors that can use the lasso or a version of it like the elastic net. The simple model will be less computationally expensive because the residuals will be larger than starting out with a more complex model (i.e. interactions, polynomials, splines, tree-based, arima). 
2) Derive the enveloped data.
3) Use the enveloped data with a more complex model.
4) Compare the out-of-sample prediction error between the model in 3) and the same complexity model on the original (non-enveloped) data that were used in 1).

What if y based is on less complex model? 

### Reminder On Packs Used

```{r}
library(tidyverse)
library(data.table)
library(furrr)
```

### Linear For Simple - Quadratic For More Complex Model 

```{r}
envDatExRandomID <- function(n, type = "Bike") {
  hrMaxBy60Dat |> filter(exercise_type == type) |> 
  filter(dataID == unique(dataID)[n]) |>  
  rowid_to_column(var = "rowID")
} 
# hrMaxBy60Dat |> count(exercise_type, dataID) |> count(exercise_type)
envDat <- envDatExRandomID(n = sample(1:65, size = 1), type = sample(unique(hrMaxBy60Dat$exercise_type), size = 1))

# or for specific dataID plot:
envDatExSpecID <- function(testID) {
  hrMaxBy60Dat |> filter(dataID == testID) |> 
  rowid_to_column(var = "rowID")
} 
envDat <- envDatExSpecID(testID = "3CF841DF-B0EC-4A47-6672-920B1003D908")
# 302FF196-C568-4CB9-A75E-B28C1AE0D808
# 33990745-D8AA-4BB8-7F79-BA17E24DD908 # starting from quadratic is better in this case
# 67389BE6-38C8-4E05-49D7-EA6E660ED908

# 
envDat_1 <- envDat |> 
  filter(t > quantile(envDat$t)[2], t < quantile(envDat$t)[3]+100)
envDat_2 <- envDat |> 
  filter(t > quantile(envDat$t)[3]+100, t < quantile(envDat$t)[4])

cat(unique(envDat_1$dataID), "\n")

modDat1 <- lm(hr ~ poly(t, degree = 1), data = envDat_1)
#
modDat1_dt <- envDat_1 |> 
  mutate(yHat = as.numeric(fitted(modDat1))) |> 
  rename(y = hr, id = rowID) |> 
  select(id, t, y, yHat) |> 
  data.table::as.data.table()

modDat1_dt2 <- response_vec(y = modDat1_dt$y, yhat = modDat1_dt$yHat, id = modDat1_dt$id, nEnv = 25)
# nEnv = 25

dev.off()

modDat1_dt2[as.data.table(envDat_1 |> rename(id = rowID))[,c("id","t")], on = .(id)] |> 
  with({
    # browser()
  # simple linear:
  # modDat1 was run above and contains the linear model upon which the enveloped y values are derived
  # y enveloped is accessed in this environment
  # out-of-sample dataset:
  out_data <- envDat_2 |> rename(y = hr)
  
  ### more complex:
  # quadratic:
  modDatEx1_y_quad <- lm(y ~ poly(t, degree = 2), data = envDat_1 |> rename(y = hr))
  modDatEx1_yEnv_quad <- lm(yEnv ~ poly(t, degree = 2), data = data.table(t, yEnv))
  # out-of-sample predictions:
  modDatEx1_y_quad_preds <- predict(modDatEx1_y_quad, newdata = out_data)
  modDatEx1_yEnv_quad_preds <- predict(modDatEx1_yEnv_quad, newdata = out_data)
  
  # # zoom on out of sample:
  # plot(out_data$t, out_data$y, type = "p", ylim = c(min(out_data$y), max(out_data$y)), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  # x_ticks <- axis(1, at = seq(min(out_data$t), max(out_data$t), by = round(range(out_data$t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  # y_ticks <- axis(2, at = seq(min(out_data$y), max(out_data$y), by = round(range(out_data$y)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  # points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  # # prediction lines:
  # lines(out_data$t, modDatEx1_y_quad_preds, col = "darkorange", lwd = 3)
  # lines(out_data$t, modDatEx1_yEnv_quad_preds, col = "darkgreen", lwd = 3)
  # overall visual:
  plot(c(t,out_data$t), c(y,out_data$y), type = "p", ylim = c(min(c(y,out_data$y)), max(c(y,out_data$y))), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(c(t,out_data$t)), max(c(t,out_data$t)), by = round(range(c(t,out_data$t))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(y,out_data$y)), max(c(y,out_data$y)), by = round(range(c(y,out_data$y))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  lines(c(envDat_1$t, out_data$t), c(fitted(modDatEx1_y_quad), modDatEx1_y_quad_preds), col = "darkorange", lwd = 3)
  lines(c(t,out_data$t), c(fitted(modDatEx1_yEnv_quad), modDatEx1_yEnv_quad_preds), col = "darkgreen", lwd = 3)
  title("poly 2")
  })

# 3CF841DF-B0EC-4A47-6672-920B1003D908 # yEnv lost
# 080162B2-B6A1-459D-2012-5C773F5CD908  # yEnv lost
# 458AD82B-2435-4925-6D95-2C9A3E74D908 # nice win for yEnv
# 200DB252-3839-43C6-78C7-36C96E02D908  # nice win for yEnv
# 2EDF2EB7-3FBB-4339-8F65-670DBBCED808  # nice win for yEnv
# predictions are more balanced and calibrated
# 7B5EE0AC-2D66-4B28-BD08-B70EFC20D908 # weird case, different process, can be removed
# 856BACE7-3A0C-4853-DC14-F3A1B725D908 # weird, high jump 
# B7E6B57B-FDB0-4FAD-2B9F-04055332D908 # jumps 
# E2968CDC-28C3-403C-CCDA-5ABE522BD908 # quadratic on y is highly sensitive to the last 2 observations in the training set when a jump occurred 
# 710DEF26-0889-414D-4D01-71E4021DD908 # yEnv loses to y with the default nEnv=NA value. There are 2,300,837 rows with NA nEnv default! Takes long time to plot. yEnv gets almost same performance as y with nEnv=25.  


# stats on all tests:

tictoc::tic()
results1 <- hrMaxBy60Dat |> nest(data = c(t, hr, exercise_type)) |> 
  filter(row_number() != 223) |> 
  mutate(chck = map(data, envDatEx))
tictoc::toc()
# 481.279 sec # could run in parallel
# error in index 223
# Caused by error in `seq.default()`:
# ! 'by' argument is much too small

# check with browser within the envDatEx function below:
# results1_nEnv1 <- hrMaxBy60Dat |> nest(data = c(t, hr, exercise_type)) |> 
#   filter(row_number() != 223) |> 
#   slice_head(n = 3) |> 
#   mutate(chck = map(data, envDatEx, nEnv = 100))
# ok

envDatEx <- function(testData, polyDegree1 = 1, polyDegree2 = 2, ...) {
  # Note: The nEnv argument to response_vec function is passed through the ellipsis.
  # browser()
  envDat <- testData |>  
  rowid_to_column(var = "id")
  
  envDat_1 <- envDat |> 
  filter(t > quantile(envDat$t)[2], t < quantile(envDat$t)[3]+100)
envDat_2 <- envDat |> 
  filter(t > quantile(envDat$t)[3]+100, t < quantile(envDat$t)[4])
  
  modDat1 <- lm(hr ~ poly(t, degree = polyDegree1), data = envDat_1)
  # 
  modDat1_dt <- envDat_1 |> 
  mutate(yHat = as.numeric(fitted(modDat1))) |> 
  rename(y = hr) |> 
  select(id, t, y, yHat) |> 
  data.table::as.data.table()

  modDat1_dt2 <- response_vec(y = modDat1_dt$y, yhat = modDat1_dt$yHat, id = modDat1_dt$id, ...)
  
  modDat1_dt2 <- modDat1_dt2[as.data.table(envDat_1)[,c("id","t")], on = .(id)] 
  # out-of-sample dataset:
  out_data <- envDat_2 |> rename(y = hr)
  
  ### more complex:
  # 
  modDatEx1_y <- lm(y ~ poly(t, degree = polyDegree2), data = envDat_1 |> rename(y = hr))
  modDatEx1_yEnv <- lm(yEnv ~ poly(t, degree = polyDegree2), data = modDat1_dt2)
  # out-of-sample predictions:
  modDatEx1_y_preds <- predict(modDatEx1_y, newdata = out_data)
  modDatEx1_yEnv_preds <- predict(modDatEx1_yEnv, newdata = out_data)
  # stats:
  return(tibble(
  nY = nrow(envDat_1), 
  mseYtrain = mean((fitted(modDatEx1_y) - envDat_1$hr)^2),
  mseYtest = mean((modDatEx1_y_preds - out_data$y)^2),
  nYEnv = nrow(modDat1_dt2),
  mseYEnvtrain = mean((fitted(modDatEx1_yEnv) - modDat1_dt2$y)^2),
  mseYEnvtest = mean((modDatEx1_yEnv_preds - out_data$y)^2)
  ))
} 

results1 |> select(-data) |> unnest(chck) |> 
  summarise(across(where(is.numeric), mean))
results1 |> select(-data) |> unnest(chck) |> 
  filter(mseYtest <= mseYEnvtest)
results1 |> select(-data) |> unnest(chck) |> 
  mutate(chck = abs(mseYtest - mseYEnvtest)) |> 
  filter(chck > 100) |> 
  filter(mseYtest <= mseYEnvtest) |> View()
results1 |> select(-data) |> unnest(chck) |> 
  mutate(chck = abs(mseYtest - mseYEnvtest)) |> 
  filter(chck > 1e3) |> 
  filter(mseYtest > mseYEnvtest)

# https://stackoverflow.com/questions/3541713/how-can-i-plot-two-histograms-together-in-r
results1 |> select(-data) |> unnest(chck) |> select(mseYtest, mseYEnvtest) |> #skimr::skim()
  with({
    p1 <- hist(mseYtest, breaks = 50)
    p2 <- hist(mseYEnvtest, breaks = 50)
    plot(p1, col = rgb(0,0,1,1/4), ylim = c(0,40))
    plot(p2, col = rgb(1,0,0,1/4), add = TRUE)})

# 302FF196-C568-4CB9-A75E-B28C1AE0D808

### rerun with nEnv parameter:
library(furrr)
plan(multisession)

tictoc::tic()
results1_nEnv200 <- hrMaxBy60Dat |> nest(data = c(t, hr, exercise_type)) |>
  filter(row_number() != 223) |>
  mutate(chck = future_map(data, envDatEx, nEnv = 200))
tictoc::toc()
# 10.28 sec
results1_nEnv200 |> select(-data) |> unnest(chck) |> 
  summarise(across(where(is.numeric), mean))

tictoc::tic()
results1_nEnv100 <- hrMaxBy60Dat |> nest(data = c(t, hr, exercise_type)) |>
  filter(row_number() != 223) |>
  mutate(chck = future_map(data, envDatEx, nEnv = 100))
tictoc::toc()
# 13.6 sec
results1_nEnv100 |> select(-data) |> unnest(chck) |> 
  summarise(across(where(is.numeric), mean))

tictoc::tic()
results1_nEnv50 <- hrMaxBy60Dat |> nest(data = c(t, hr, exercise_type)) |>
  filter(row_number() != 223) |>
  mutate(chck = future_map(data, envDatEx, nEnv = 50))
tictoc::toc()
# 12.67 sec
results1_nEnv50 |> select(-data) |> unnest(chck) |> 
  summarise(across(where(is.numeric), mean))

tictoc::tic()
results1_nEnv25 <- hrMaxBy60Dat |> nest(data = c(t, hr, exercise_type)) |>
  filter(row_number() != 223) |>
  mutate(chck = future_map(data, envDatEx, nEnv = 25))
tictoc::toc()
# 8.22 sec
results1_nEnv25 |> select(-data) |> unnest(chck) |> 
  summarise(across(where(is.numeric), mean))

# vis of largest wins for the usual y based process :
tempIDs <- results1_nEnv100 |> select(-data) |> unnest(chck) |> 
  filter(mseYtest < mseYEnvtest) |> # 172 cases out of 546 or 31.5%
  mutate(chck = abs(mseYtest - mseYEnvtest)) |> 
  filter(chck > 25) |> # range of +/- 5 i.e. sqrt(25) # 43 cases or 7.8% of total 546
  pull(dataID)

### repeat code from above for the visual:
# or for specific dataID plot:
envDatExSpecID <- function(testID) {
  hrMaxBy60Dat |> filter(dataID == testID) |> 
  rowid_to_column(var = "rowID")
} 

dev.off()

for (i in seq_along(tempIDs)) {
envDat <- envDatExSpecID(testID = tempIDs[i])

envDat_1 <- envDat |> 
  filter(t > quantile(envDat$t)[2], t < quantile(envDat$t)[3]+100)
envDat_2 <- envDat |> 
  filter(t > quantile(envDat$t)[3]+100, t < quantile(envDat$t)[4])

cat(unique(envDat_1$dataID), "\n")

modDat1 <- lm(hr ~ poly(t, degree = 1), data = envDat_1)
#
modDat1_dt <- envDat_1 |> 
  mutate(yHat = as.numeric(fitted(modDat1))) |> 
  rename(y = hr, id = rowID) |> 
  select(id, t, y, yHat) |> 
  data.table::as.data.table()

modDat1_dt2 <- response_vec(y = modDat1_dt$y, yhat = modDat1_dt$yHat, id = modDat1_dt$id, nEnv = 100) # as in results1_nEnv100 object result

modDat1_dt2[as.data.table(envDat_1 |> rename(id = rowID))[,c("id","t")], on = .(id)] |> 
  with({

  out_data <- envDat_2 |> rename(y = hr)
  
  ### more complex:
  # quadratic:
  modDatEx1_y_quad <- lm(y ~ poly(t, degree = 2), data = envDat_1 |> rename(y = hr))
  modDatEx1_yEnv_quad <- lm(yEnv ~ poly(t, degree = 2), data = data.table(t, yEnv))
  # out-of-sample predictions:
  modDatEx1_y_quad_preds <- predict(modDatEx1_y_quad, newdata = out_data)
  modDatEx1_yEnv_quad_preds <- predict(modDatEx1_yEnv_quad, newdata = out_data)
  
  plot(c(t,out_data$t), c(y,out_data$y), type = "p", ylim = c(min(c(y,out_data$y)), max(c(y,out_data$y))), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(c(t,out_data$t)), max(c(t,out_data$t)), by = round(range(c(t,out_data$t))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(y,out_data$y)), max(c(y,out_data$y)), by = round(range(c(y,out_data$y))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.08), bg = scales::alpha("black", 0.08), pch = 21)
  lines(c(envDat_1$t, out_data$t), c(fitted(modDatEx1_y_quad), modDatEx1_y_quad_preds), col = "darkorange", lwd = 3)
  lines(c(t,out_data$t), c(fitted(modDatEx1_yEnv_quad), modDatEx1_yEnv_quad_preds), col = "darkgreen", lwd = 3)
  title(paste0("poly 2 - ", tempIDs[i]))
  })
}
# difference do not seem significant i.e. the usual y process has won out of luck
rm(tempIDs)
```

### Linear For Simple - Cubic For More Complex Model

```{r}
# stats on all tests:
library(furrr)
plan(multisession)
# function envDatEx is defined above
tictoc::tic()
results2 <- hrMaxBy60Dat |> nest(data = c(t, hr, exercise_type)) |> 
  filter(row_number() != 223) |> 
  mutate(chck = future_map(data, envDatEx, polyDegree2 = 3))
tictoc::toc()
# 34.278 sec 

results2 |> select(-data) |> unnest(chck) |> 
  summarise(across(where(is.numeric), mean))
```

### Quadratic For Simple - Cubic For More Complex Model

```{r}
# stats on all tests:
library(furrr)
plan(multisession)
# function envDatEx is defined above
tictoc::tic()
results3 <- hrMaxBy60Dat |> nest(data = c(t, hr, exercise_type)) |> 
  filter(row_number() != 223) |> 
  mutate(chck = future_map(data, envDatEx, polyDegree1 = 2, polyDegree2 = 3))
tictoc::toc()
# 48.538 sec # more y enveloped values are generated due to starting with more complex (smaller errors) model

results3 |> select(-data) |> unnest(chck) |> 
  summarise(across(where(is.numeric), mean))
```

### Quadratic For Simple - Quadratic For More Complex Model 

Any difference? Cos when Linear For Simple - Linear For More Complex Model the result is same.

```{r}
# stats on all tests:
library(furrr)
plan(multisession)
# function envDatEx is defined above
tictoc::tic()
results4 <- hrMaxBy60Dat |> nest(data = c(t, hr, exercise_type)) |> 
  filter(row_number() != 223) |> 
  mutate(chck = future_map(data, envDatEx, polyDegree1 = 2, polyDegree2 = 2))
tictoc::toc()
# 42.67 sec
results4 |> select(-data) |> unnest(chck) |> 
  summarise(across(where(is.numeric), mean))
# # A tibble: 1 × 6
#      nY mseYtrain mseYtest   nYEnv mseYEnvtrain mseYEnvtest
#   <dbl>     <dbl>    <dbl>   <dbl>        <dbl>       <dbl>
# 1  64.4      35.6     539. 127067.         8.03        536.
# No diff on test set. 
# Using the nEnv parameter:
tictoc::tic()
results4 <- hrMaxBy60Dat |> nest(data = c(t, hr, exercise_type)) |> 
  filter(row_number() != 223) |> 
  mutate(chck = future_map(data, envDatEx, polyDegree1 = 2, polyDegree2 = 2, nEnv = 25))
tictoc::toc()
# 8.4 sec
results4 |> select(-data) |> unnest(chck) |> 
  summarise(across(where(is.numeric), mean))
# No diff on test set.
rm(results4)
```

### Linear For Simple - Auto Arima For More Complex Model 

```{r}
### repeat code from above for the visual:
# or for specific dataID plot:
# envDatExSpecID <- function(testID) {
#   hrMaxBy60Dat |> filter(dataID == testID) |> 
#   rowid_to_column(var = "rowID")
# } 
# dev.off()

for (p in 1:10) {

envDat <- envDatExSpecID(testID = unique(hrMaxBy60Dat$dataID)[p]) # 16 a nice example

cat(unique(envDat$dataID), "\n")
cat(p, "\n")

envDat_1 <- envDat |> 
  filter(t > quantile(envDat$t)[2], t < quantile(envDat$t)[3]+100)
envDat_2 <- envDat |> 
  filter(t > quantile(envDat$t)[3]+100, t < quantile(envDat$t)[4])

# modDat1 <- lm(hr ~ poly(t, degree = 1), data = envDat_1) # this simple model though is not of same class as the complex one i.e. it uses a predictor variable while the Arima does not (time series models do not use time explicitly but implicitly through the sequence of observations). Hence this approach would not comply with the idea of using the same set of predictors between the simple and the complex model. 
modDat1 <- forecast::Arima(y = envDat_1$hr, order = c(0,1,0))
# modDat1 <- forecast::auto.arima(y = envDat_1$hr)
#
modDat1_dt <- envDat_1 |> 
  mutate(yHat = as.numeric(fitted(modDat1))+1e-10) |> # 1e-10 to account for zero error of the naive simple model (or other arima type model) when the data pattern looks like a step function 
  rename(y = hr, id = rowID) |> 
  select(id, t, y, yHat) |> 
  data.table::as.data.table()

modDat1_dt2 <- response_vec(y = modDat1_dt$y, yhat = modDat1_dt$yHat, id = modDat1_dt$id, nEnv = 500) # 250 also seemed ok # 25 were too little it seems as the prediction did not change much by taking different sample per id as below (either random or same row)

# taking a random row per id:
modDat1_dt2_1PerID <- modDat1_dt2[modDat1_dt2[, .I[sample(.N, 1)], by = id][[2]]]
# taking each specific row per id: Does not make sense to do that cos the total number of yenv values is not the same across the y,yhat pairs. For example choosing below the index 90 produces an error because not all y,yhat pairs have 90 yenv values. As an operation also does not seem to make sense. 
# modDat1_dt2_1PerID <- modDat1_dt2[modDat1_dt2[, .I[90], by = id][[2]]]
# modDat1_dt2 |> count(id, sort = T) |> tail(1) # 77 yenv values is the minimum

# repeating 300 random samples of yenv values:
# plan(multisession)
tictoc::tic()
forcs <- future_map(1:300, function(i) {
  modDat1_dt2_1PerID <- modDat1_dt2[modDat1_dt2[, .I[sample(.N, 1)], by = id][[2]]]
  modDatEx1_yEnv_c <- forecast::auto.arima(y = modDat1_dt2_1PerID$yEnv)
  modDatEx1_yEnv_c
  data.table(sampleID = i, h = 1:length(envDat_2$hr), forcs = as.numeric(forecast::forecast(modDatEx1_yEnv_c, h=length(envDat_2$hr))$mean))
}, .options = furrr_options(seed = TRUE)) |> rbindlist() |> group_by(h) |> summarise(forcsMean = mean(forcs))
tictoc::toc()
# 15, 8 sec


modDat1_dt2_1PerID[as.data.table(envDat_1 |> rename(id = rowID))[,c("id","t")], on = .(id)] |>
  with({
# browser()
  out_data <- envDat_2 |> rename(y = hr)
  
  ### more complex:
  modDatEx1_y_c <- forecast::auto.arima(y = envDat_1 |> rename(y = hr) |> pull(y))
  # modDatEx1_yEnv_c <- forecast::auto.arima(y = yEnv)
  # out-of-sample predictions:
  modDatEx1_y_c_preds <- as.numeric(forecast::forecast(modDatEx1_y_c, h=length(out_data$y))$mean)
  modDatEx1_yEnv_c_preds <- forcs$forcsMean
  # modDatEx1_yEnv_c_preds <- as.numeric(forecast::forecast(modDatEx1_yEnv_c, h=length(out_data$y))$mean)
  
  plot(c(t,out_data$t), c(y,out_data$y), type = "p", ylim = c(min(c(y,out_data$y)), max(c(y,out_data$y))), xlab = "time (sec)", ylab = "y", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.08), bg = scales::alpha("steelblue", 0.08), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(c(t,out_data$t)), max(c(t,out_data$t)), by = round(range(c(t,out_data$t))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(y,out_data$y)), max(c(y,out_data$y)), by = round(range(c(y,out_data$y))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.3), bg = scales::alpha("black", 0.3), pch = 21)
  lines(c(envDat_1$t, out_data$t), c(fitted(modDatEx1_y_c), modDatEx1_y_c_preds), col = "darkorange", lwd = 3)
  lines(out_data$t, modDatEx1_yEnv_c_preds, col = "darkgreen", lwd = 3)
  # lines(c(t,out_data$t), c(fitted(modDatEx1_yEnv_c), modDatEx1_yEnv_c_preds), col = "darkgreen", lwd = 3)
  title(paste0("auto arima - ", envDat$dataID[1]))
  })
}

# stats on all tests:

plan(multisession)
tictoc::tic()
results4_lm <- hrMaxBy60Dat |> nest(data = c(t, hr, exercise_type)) |> 
  filter(row_number() != 223) |> 
  mutate(chck = future_map(data, envDatEx, nEnv = 500))
tictoc::toc()
# 83 min

# https://stackoverflow.com/questions/61506909/nested-furrrfuture-map
future::plan(
      list(
        future::tweak(
          future::multisession, 
          workers = 2), 
        future::tweak(
          future::multisession,
          workers = 4)
        )
      )
tictoc::tic()
results4_naive <- hrMaxBy60Dat |> nest(data = c(t, hr, exercise_type)) |> 
  filter(row_number() != 223) |> 
  mutate(chck = future_map(data, envDatEx, step1 = "naive", nEnv = 500))
tictoc::toc()
# 76 min

plan(multisession)
tictoc::tic()
results4_auto <- hrMaxBy60Dat |> nest(data = c(t, hr, exercise_type)) |> 
  filter(row_number() != 223) |> 
  mutate(chck = map(data, envDatEx, step1 = "auto", nEnv = 500))
tictoc::toc()
# 80 min

envDatEx <- function(testData, step1 = c("lm","naive","auto"), step2 = "auto", resamples = 300, ...) {
  # step2 argument is actually not used 
  # Note: The nEnv argument to response_vec function is passed through the ellipsis.

  envDat <- testData |>  
  rowid_to_column(var = "id")
  
  envDat_1 <- envDat |> 
  filter(t > quantile(envDat$t)[2], t < quantile(envDat$t)[3]+100)
envDat_2 <- envDat |> 
  filter(t > quantile(envDat$t)[3]+100, t < quantile(envDat$t)[4])

  modDat1 <- switch(match.arg(step1), 
                    lm = lm(hr ~ poly(t, degree = 1), data = envDat_1),
                    naive = forecast::Arima(y = envDat_1$hr, order = c(0,1,0)),
                    auto = forecast::auto.arima(y = envDat_1$hr))
  
  # 
  modDat1_dt <- envDat_1 |> 
  mutate(yHat = as.numeric(fitted(modDat1))+1e-10) |> # 1e-10 is to account for zero error occuring when data look like a step function
  rename(y = hr) |> 
  select(id, t, y, yHat) |> 
  data.table::as.data.table()

  modDat1_dt2 <- response_vec(y = modDat1_dt$y, yhat = modDat1_dt$yHat, id = modDat1_dt$id, ...)
  
  # out-of-sample dataset:
  out_data <- envDat_2 |> rename(y = hr)
  
  ### more complex:
  forcs <- future_map(1:resamples, function(i) {
  modDat1_dt2_1PerID <- modDat1_dt2[modDat1_dt2[, .I[sample(.N, 1)], by = id][[2]]]
  modDatEx1_yEnv_c <- forecast::auto.arima(y = modDat1_dt2_1PerID$yEnv)
  data.table(sampleID = i, h = 1:length(envDat_2$hr), forcs = as.numeric(forecast::forecast(modDatEx1_yEnv_c, h=length(out_data$y))$mean))
}, .options = furrr_options(seed = TRUE)) |> 
    rbindlist() |> 
    group_by(h) |> 
    summarise(forcsMean = mean(forcs))
  # 
  modDatEx1_y_c <- forecast::auto.arima(y = envDat_1 |> rename(y = hr) |> pull(y))
  # modDatEx1_yEnv_c <- forecast::auto.arima(y = yEnv)
  # out-of-sample predictions:
  modDatEx1_y_c_preds <- as.numeric(forecast::forecast(modDatEx1_y_c, h=length(out_data$y))$mean)
  modDatEx1_yEnv_c_preds <- forcs$forcsMean

  # stats:
  return(tibble(
  nY = nrow(envDat_1), 
  mseYtrain = mean((fitted(modDat1) - envDat_1$hr)^2),
  mseYtest = mean((modDatEx1_y_c_preds - out_data$y)^2),
  # nYEnv = nrow(modDat1_dt2), # leaving out due to no single measure from the many resamples
  # mseYEnvtrain = mean((fitted(modDatEx1_yEnv) - modDat1_dt2$y)^2), # leaving out due to no single measure from the many resamples
  mseYEnvtest = mean((modDatEx1_yEnv_c_preds - out_data$y)^2)
  ))
} 

results4_lm |> select(-data) |> unnest(chck) |> 
  summarise(across(where(is.numeric), mean))
# # A tibble: 1 × 4
#      nY mseYtrain mseYtest mseYEnvtest
#   <dbl>     <dbl>    <dbl>       <dbl>
# 1  64.4      48.3     198.        195.
# same performance between y and yenv cases. Smaller mse due to more complex model i.e. arima vs quadratic previously

results4_naive |> select(-data) |> unnest(chck) |> 
  summarise(across(where(is.numeric), mean))
# # A tibble: 1 × 4
#      nY mseYtrain mseYtest mseYEnvtest
#   <dbl>     <dbl>    <dbl>       <dbl>
# 1  64.4      11.5     198.        202.

results4_auto |> select(-data) |> unnest(chck) |> 
  summarise(across(where(is.numeric), mean))
# # A tibble: 1 × 4
#      nY mseYtrain mseYtest mseYEnvtest
#   <dbl>     <dbl>    <dbl>       <dbl>
# 1  64.4      10.8     198.        207.
```

Due to the nature of the data i.e. 1 minute sliding windows every 3 or so secs, y values or the max heart rate does not change much and thus y is very much like a step function with the same value repeating. This makes the problem have zero or small errors so that yenv values are not that different from the original y values. That's why results between y and yenv approaches do not differ. The arima models also capture this data behavior. Because of this there's also not much room for smoothness to be leveraged when deriving the yenv values. 

This dataset is not really ideal for this testing. Better to test on raw data i.e. not on sliding windows with stats that don't change much from one window to the next. 

Below the approach with first fitting the desired (i.e. complex) model, then extracting the residuals and lastly fitting time to the enveloped values by using a smoothing spline to get the predictions. 

```{r}
dev.off()
plan(multisession)
tictoc::tic()
results5 <- hrMaxBy60Dat |> group_by(dataID) |> nest(data = c(t, hr, exercise_type)) |> 
  filter(row_number() != 223) |> 
  mutate(chck = map(data, envDatEx, cvSmooth = FALSE, plot = TRUE, nEnv = 500))
tictoc::toc()
# note that I use group_by(dataID) in order to accedd to the dataID and use in the plot title

results5 <- hrMaxBy60Dat |> nest(data = c(t, hr, exercise_type)) |> 
  filter(row_number() != 223) |> 
  # group_by(dataID) |> # only needed for plotting # future_map does not execute in parallel
  mutate(chck = future_map(data, envDatEx, cvSmooth = FALSE, dfSmooth = 6, plot = FALSE, zoom = FALSE, nEnv = 25)) # future_map

# graphical checks:
hrMaxBy60Dat |> nest(data = c(t, hr, exercise_type)) |> 
  filter(row_number() != 223) |> 
  filter(row_number() == 87) |> # 28 # large errors are due to the jump cases in data which are not modeled when training the starting model
  group_by(dataID) |> 
  mutate(chck = map(data, envDatEx, cvSmooth = FALSE, dfSmooth = 6, plot = TRUE, zoom = TRUE, nEnv = 25))

hrMaxBy60Dat |> nest(data = c(t, hr, exercise_type)) |> 
  filter(dataID %in% results5$dataID) |> 
  slice_sample(n=1) |> 
  group_by(dataID) |> 
  mutate(chck = map(data, envDatEx, cvSmooth = FALSE, dfSmooth = 6, plot = TRUE, zoom = TRUE, nEnv = 25))

# envDatEx is redefined here:
envDatEx <- function(testData, step1 = "auto", step2 = "sm", cvSmooth, dfSmooth, plot, zoom, ...) {
  # step1 argument is actually not used
  # Note: The nEnv argument to response_vec function is passed through the ellipsis.
  # When plot & zoom are TRUE, the data needs to be grouped so that the group id is printed on the plot.
# browser()
  # https://stackoverflow.com/questions/28256576/how-to-write-an-r-function-that-displays-plots-sequentially
  
  envDat <- testData |>  
  rowid_to_column(var = "id")
  
  envDat_1 <- envDat |> 
  filter(t > quantile(envDat$t)[2], t < quantile(envDat$t)[3]+100)
  envDat_2 <- envDat |> 
  filter(t > quantile(envDat$t)[3]+100, t < quantile(envDat$t)[4])

  modDat1 <- switch(match.arg(step1), 
                    lm = lm(hr ~ poly(t, degree = 1), data = envDat_1),
                    naive = forecast::Arima(y = envDat_1$hr, order = c(0,1,0)),
                    auto = forecast::auto.arima(y = envDat_1$hr))
  
  # 
  modDat1_dt <- envDat_1 |> 
  mutate(yHat = as.numeric(fitted(modDat1))+1e-2) |> # 1e-10 is to account for zero error occuring when data look like a step function # but such small diffs can lead to error when calling the response_vec function below. 
  rename(y = hr) |> 
  select(id, t, y, yHat) |> 
  data.table::as.data.table()

  modDat1_dt2 <- response_vec(y = modDat1_dt$y, yhat = modDat1_dt$yHat, id = modDat1_dt$id, ...)
  
  # out-of-sample dataset:
  out_data <- envDat_2 |> rename(y = hr)
  
  # smoothing splines:
  modDat2 <- stats::smooth.spline(x = modDat1_dt2[modDat1_dt[,c("id","t")], on = .(id)][,.(x = t, y = yEnv)], cv = cvSmooth, df = dfSmooth, tol = 0.00001)
  
  # out-of-sample predictions:
  # y based:
  modDatEx1_y_c_preds <- as.numeric(forecast::forecast(modDat1, h=length(out_data$t))$mean)
  # yEnv based:
  modDatEx1_yEnv_c_preds <- predict(modDat2, x = out_data$t)$y
  
  if (plot) { 
    
    if (zoom) {
      plot(c(envDat_1$t, out_data$t), c(envDat_1$hr, out_data$y), type = "p", ylim = c(min(c(envDat_1$hr, out_data$y, modDatEx1_y_c_preds, modDatEx1_yEnv_c_preds)), max(c(envDat_1$hr, out_data$y, modDatEx1_y_c_preds, modDatEx1_yEnv_c_preds))), xlab = "time (sec)", ylab = "yEnv", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.3), bg = scales::alpha("steelblue", 0.3), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(c(envDat_1$t, out_data$t)), max(c(envDat_1$t, out_data$t)), by = round(range(c(envDat_1$t, out_data$t))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(envDat_1$hr, out_data$y, modDatEx1_y_c_preds, modDatEx1_yEnv_c_preds)), max(c(envDat_1$hr, out_data$y, modDatEx1_y_c_preds, modDatEx1_yEnv_c_preds)), by = round(range(c(envDat_1$hr, out_data$y, modDatEx1_y_c_preds, modDatEx1_yEnv_c_preds))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.3), bg = scales::alpha("black", 0.3), pch = 21)
  lines(envDat_1$t, as.numeric(fitted(modDat1)), col = "darkorange", lwd = 3)
  lines(modDat1_dt2[modDat1_dt[,c("id","t")], on = .(id)][,t], as.numeric(fitted(modDat2)), col = "darkgreen", lwd = 3)
  points(out_data$t, modDatEx1_y_c_preds, lwd = 3, col = scales::alpha("darkorange", 0.5), bg = scales::alpha("darkorange", 0.5), pch = 21)
  points(out_data$t, modDatEx1_yEnv_c_preds, lwd = 3, col = scales::alpha("darkgreen", 0.5), bg = scales::alpha("darkgreen", 0.5), pch = 21)
  points(modDat1_dt2[modDat1_dt[,c("id","t")], on = .(id)][,c('t','yEnv')], lwd = 3, col = scales::alpha("darkgreen", 0.01), bg = scales::alpha("darkgreen", 0.01), pch = 21)
  title(paste0("test id - ", cur_group()$dataID))
  # the yEnv values are super close due to arima type of model, so that smoothness is not actually leveraged.
    } else {
    plot(envDat$t, envDat$hr, type = "p", ylim = c(min(c(envDat$hr, modDatEx1_y_c_preds, modDatEx1_yEnv_c_preds)), max(c(envDat$hr, modDatEx1_y_c_preds, modDatEx1_yEnv_c_preds))), xlab = "time (sec)", ylab = "yEnv", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.3), bg = scales::alpha("steelblue", 0.3), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(envDat$t), max(envDat$t), by = round(range(envDat$t)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(envDat$hr, modDatEx1_y_c_preds, modDatEx1_yEnv_c_preds)), max(c(envDat$hr, modDatEx1_y_c_preds, modDatEx1_yEnv_c_preds)), by = round(range(c(envDat$hr, modDatEx1_y_c_preds, modDatEx1_yEnv_c_preds))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out_data$t, out_data$y, col = scales::alpha("black", 0.3), bg = scales::alpha("black", 0.3), pch = 21)
  lines(envDat_1$t, as.numeric(fitted(modDat1)), col = "darkorange", lwd = 3)
  lines(modDat1_dt2[modDat1_dt[,c("id","t")], on = .(id)][,t], as.numeric(fitted(modDat2)), col = "darkgreen", lwd = 3)
  points(out_data$t, modDatEx1_y_c_preds, lwd = 3, col = scales::alpha("darkorange", 0.5), bg = scales::alpha("darkorange", 0.5), pch = 21)
  points(out_data$t, modDatEx1_yEnv_c_preds, lwd = 3, col = scales::alpha("darkgreen", 0.5), bg = scales::alpha("darkgreen", 0.5), pch = 21)
  title(paste0("test id - ", cur_group()$dataID))
    }
  }
# https://stackoverflow.com/questions/53855897/accessing-grouping-variables-in-purrrmap-with-nested-dataframes
  
  # stats:
  return(tibble(
  nY = nrow(envDat_1), 
  mseYtrain = mean((fitted(modDat1) - envDat_1$hr)^2),
  mseYtest = mean((modDatEx1_y_c_preds - out_data$y)^2),
  nYEnv = nrow(modDat1_dt2), 
  mseYEnvtrain = mean((fitted(modDat2) - modDat1_dt2$y)^2), 
  mseYEnvtest = mean((modDatEx1_yEnv_c_preds - out_data$y)^2)
  ))
  oask <- devAskNewPage(TRUE)
  on.exit(devAskNewPage(oask))
} 

results5 |> select(-data) |> unnest(chck) |>
  # ungroup() |>  
  summarise(across(where(is.numeric), mean))
# nEnv=500 # cv = FALSE
# # A tibble: 1 × 6
#      nY mseYtrain mseYtest  nYEnv mseYEnvtrain mseYEnvtest
#   <dbl>     <dbl>    <dbl>  <dbl>        <dbl>       <dbl>
# 1  64.4      10.8     198. 26887.        0.131      14660.
     
# nEnv=500 # df=2
# # A tibble: 1 × 6
#      nY mseYtrain mseYtest  nYEnv mseYEnvtrain mseYEnvtest
#   <dbl>     <dbl>    <dbl>  <dbl>        <dbl>       <dbl>
# 1  64.4      10.8     198. 26887.         46.8        253.

# nEnv=500 # df=12
# # A tibble: 1 × 6
#      nY mseYtrain mseYtest  nYEnv mseYEnvtrain mseYEnvtest
#   <dbl>     <dbl>    <dbl>  <dbl>        <dbl>       <dbl>
# 1  64.4      10.8     198. 26887.         3.61        558.

# nEnv=25 # df=3
# # A tibble: 1 × 6
#      nY mseYtrain mseYtest nYEnv mseYEnvtrain mseYEnvtest
#   <dbl>     <dbl>    <dbl> <dbl>        <dbl>       <dbl>
# 1  64.4      10.8     198. 1669.         30.2        281.

# nEnv=25 # df=6
# # A tibble: 1 × 6
#      nY mseYtrain mseYtest nYEnv mseYEnvtrain mseYEnvtest
#   <dbl>     <dbl>    <dbl> <dbl>        <dbl>       <dbl>
# 1  64.4      10.8     198. 1669.         12.0        380.

# nEnv=25 # df=6
results5 |> select(-data) |> unnest(chck) |>
  rowid_to_column() |> arrange(-mseYEnvtest) |> filter(mseYEnvtest < 1e2) |> 
  summarise(across(where(is.numeric), mean))
# # A tibble: 1 × 7
#   rowid    nY mseYtrain mseYtest nYEnv mseYEnvtrain mseYEnvtest
#   <dbl> <dbl>     <dbl>    <dbl> <dbl>        <dbl>       <dbl>
# 1  273.  64.3      3.94     23.1 1669.         4.44        12.2
# Significant improvement when focusing on the non-jump cases (which we are not modeling)
# Error reduction almost by half. 
  
# nEnv=NA # df=2

# nEnv=NA # df=12
```


Findings: The envelope approach improves the predictive performance over the usual approach when the assumed data generating process stays the same on the test set period. When a model is trained or fit on a training data set, there's an implicit assumption of a data generating mechanism that we would like to model. The prediction error, from which the envelope response values are derived, is accounting for this mechanism. If the mechanism changes in the test set period then the envelope does not add any value since it has not accounted for this change. 
In this example data, the assumption is that we model a gradual increase in exercise intensity which is apparent in the training set by the gradual increase of the 1 minute interval maximum heart rate. There is no modeling or information that we have used in the model on the event of large spikes in the maximum heart rate due to a sudden exercise intensity increase by the user. Therefore when these spikes occur in the test period, the data generating mechanism has changed and the envelope approach does not perform well since it is 'unaware' of this change i.e. the prediction error did not account for this. The largest errors of the envelope approach occur on these cases. 
With time series models, the residual error can be small, so a small nEnv argument might be enough to produce a good result. The spline parameter(s) (i.e. df) need to be tuned to locate the best performing spline approach using the response envelope values. 


## Data 2 Example - Import

Longitudinal data example from Harrell. 

```{r}
library(tidyverse)
library(data.table)
# data and code taken from: 
# /Users/aristidiskoitsanos/Desktop/ari/IBM_import/DS_resources_last/Harrell_resources/Regression Modeling Strategies_rms_Rpack/Notes_rms_Harrel.R
both <- read_csv("both.csv")
skimr::skim(both)
both |> count(id, sort = T) |> tail()
unique(both$week) # [1]  2  4  8 12 16
both_train1 <- both |> filter(week <= 8) 
both_test1 <- both |> filter(week == 12)
# with an additional observation:
both_train1 <- both |> filter(week <= 12)
both_test1 <- both |> filter(week == 16)
rm(both)
# dd <- datadist(both_train1)
# options(datadist = "dd")
# dd

library(rms) # maskings
library(nlme)
a1 <- Gls(twstrs ~ treat + week + twstrs0 + age + sex, 
         data = both_train1, 
         na.action = na.omit)
a1_chck <- lm(twstrs ~ treat + week + twstrs0 + age + sex, 
         data = both_train1, 
         na.action = na.omit)
# same as a1
rm(a1_chck)
a1 <- Gls(twstrs ~ treat + week + twstrs0 + age + sex, 
         data = both_train1,
         correlation = corCAR1(form = ~ week | id), 
         na.action = na.omit)

a2 <- Gls(twstrs ~ treat * rcs(week, c(3,6,10)) + rcs(twstrs0, 3) + rcs(age, 4) * sex, 
         data = both_train1,
         correlation = corCAR1(form = ~ week | id), 
         na.action = na.omit)
# c(2,4,8)

both_train1 |> na.omit() |> mutate(yhat1 = fitted(a1), yhat2 = fitted(a2)) |> rename(y = twstrs)

# ?residuals.gls
# hist(residuals(a1, type = "pearson"))
# hist(residuals(a1))
# hist(residuals(a1, type = "normalized")) # this type confirms that the covariance structure of the model has been captured in the data i.e. within id dependence like serial correlation and/or heteroscedasticity
# ACF(a1, form = ~ week | id, resType = "normalized")

modDat1_dt <- both_train1 |> 
  na.omit() |> 
  mutate(yHat = as.numeric(fitted(a1))) |> 
  rowid_to_column() |> 
  rename(y = twstrs) |> 
  data.table::as.data.table()
# id is now rowid:
modDat1_dt2 <- response_vec(y = modDat1_dt$y, yhat = modDat1_dt$yHat, id = modDat1_dt$rowid, nEnv = NA) |> rename(rowid = id)

library(furrr)
plan(multisession)

# chck on taking a random sample by rowid i.e. yenv and then joining to add the predictors:
# modDat1_dt2_1PerID <- modDat1_dt2[modDat1_dt2[, .I[sample(.N, 1)], by = rowid][[2]]]
# 
# modDat1_dt2_1PerID[modDat1_dt[,c("rowid","id","treat","age","sex","twstrs0","week")], on = .(rowid)]
# rm(modDat1_dt2_1PerID)

modDat1_dt2[modDat1_dt[,c("rowid","id","treat","age","sex","twstrs0","week")], on = .(rowid)] |> filter(id == 107) |> with({
  # browser()
  out <- both_test1 |> rename(ID = id) |> filter(ID == unique(id))
  
  # modDatEx1_yEnv_c <- Gls(yEnv ~ rcs(week, c(3,6,10)))
  # modDatEx1_yEnv_c <- Gls(yEnv ~ rcs(week, c(2,4,8,12)))
  ## modDatEx1_yEnv_c <- Gls(yEnv ~ rcs(week, c(3:6)))
  # modDatEx1_yEnv_c <- Gls(yEnv ~ rcs(week, c(3,6,9,12)))
  ## modDatEx1_yEnv_c_preds <- predict(modDatEx1_yEnv_c, newdata = out)
    
  # smoothing splines:
  ## modDatEx1_yEnv_c <- stats::smooth.spline(x = week, y = yEnv, cv = FALSE, tol = 0.00001) # GCV
  # out-of-sample predictions:
  ## modDatEx1_yEnv_c_preds <- predict(modDatEx1_yEnv_c, x = out$week)$y
  
  modDatEx1_yEnv_c <- Gls(yEnv ~ week)
  modDatEx1_yEnv_c_preds <- predict(modDatEx1_yEnv_c, newdata = out)
  
  plot(c(week, out$week), c(yEnv,out$twstrs), type = "p", ylim = c(min(c(yEnv,out$twstrs,modDatEx1_yEnv_c_preds)), max(c(yEnv,out$twstrs,modDatEx1_yEnv_c_preds))), xlab = "time (sec)", ylab = "yEnv", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.3), bg = scales::alpha("steelblue", 0.3), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(c(week, out$week)), max(c(week, out$week)), by = round(range(c(week, out$week))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(yEnv, out$twstrs, modDatEx1_yEnv_c_preds)), max(c(yEnv, out$twstrs, modDatEx1_yEnv_c_preds)), by = round(range(c(yEnv, out$twstrs, modDatEx1_yEnv_c_preds))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out$week, out$twstrs, col = scales::alpha("black", 0.3), bg = scales::alpha("black", 0.3), pch = 21)
  points(unique(week), y[cumsum(table(week))], col = scales::alpha("darkorange", 1), bg = scales::alpha("darkorange", 1), pch = 21)
  lines(week, as.numeric(fitted(modDatEx1_yEnv_c)), col = "darkorange", lwd = 3)
  points(out$week, modDatEx1_yEnv_c_preds, col = "darkgreen", lwd = 3)
  # title(paste0("auto arima - ", envDat$dataID[1]))
  })


tictoc::tic()
forcs <- future_map(1:300, function(i) {
  # random sample by rowid i.e. yenv
  modDat1_dt2_1PerID <- modDat1_dt2[modDat1_dt2[, .I[sample(.N, 1)], by = rowid][[2]]]
  # fit gls on yenv
  modDatEx1_yEnv_c <- Gls(yEnv ~ treat * rcs(week, c(3,6,10)) + rcs(twstrs0, 3) + rcs(age, 4) * sex, data = modDat1_dt2_1PerID[modDat1_dt[,c("rowid","id","treat","age","sex","twstrs0","week")], on = .(rowid)], correlation = corCAR1(form = ~ week | id), na.action = na.omit)
  # c(2,4,8)
  # predictions with subject id added
  data.table(id = both_test1$id, sampleID = i, forcs = as.numeric(predict(modDatEx1_yEnv_c, newdata = as.data.table(both_test1 |> select(id,treat,age,sex,twstrs0,week)))))
}, .options = furrr_options(seed = TRUE)) |> 
  rbindlist() |> 
  group_by(id) |> 
  summarise(forcsMean = mean(forcs))
tictoc::toc()
# 14.2 sec

both_test1 |> mutate(preds_y = as.numeric(predict(a2, newdata = as.data.table(both_test1 |> select(id,treat,age,sex,twstrs0,week))))) |> 
  inner_join(forcs |> rename(preds_yEnv = forcsMean), by = "id") |> 
  na.omit() |> 
  summarise(err_y = mean(abs(twstrs-preds_y)), err_yEnv = mean(abs(twstrs-preds_yEnv)))
# # A tibble: 1 × 2
#   err_y err_yEnv
#   <dbl>    <dbl>
# 1  6.21     6.30
# similar results; perhaps too few data i.e. only 3 observations per id
# with one more observation in training data i.e. 4:
# # A tibble: 1 × 2
#   err_y err_yEnv
#   <dbl>    <dbl>
# 1  6.36     6.33

modDat1_dt2[modDat1_dt[,c("rowid","id","treat","age","sex","twstrs0","week")], on = .(rowid)]  |> group_by(id) |> nest() |> mutate(abs_error = map(data, function(df) {
  idx <- cur_group()$id
  out <- both_test1 |> filter(id == idx)
  modDatEx1_yEnv_c <- Gls(yEnv ~ week, data = df)
  modDatEx1_yEnv_c_preds <- predict(modDatEx1_yEnv_c, newdata = out)
  abs(out$twstrs-modDatEx1_yEnv_c_preds[[1]])
})) |> select(-data) |> unnest(abs_error) |> 
  ungroup() |> # arrange(-abs_error) # c(53,84,107) # these are weird large error cases
  na.omit() |> # 3 NA cases # mae is 5.61
  filter(!(id %in% c(53,84,107))) |>
  summarise(mae = mean(abs_error))
# # A tibble: 1 × 1
#     mae
#   <dbl>
# 1  4.81
```


A different strategy: First fit the desired model (i.e. complex model and training to avoid overfitting as usual), then extract the envelope y values, and then fit a smoothing spline (which could be linear too) on the envelope y values against the time variable. The last step is done separately to each id if there are many ids. This abides to the space-time concept of a hypercube representing the data and time as the additional and necessary dimension that cuts through this hypecube and extrapolates into the future. Time is a necessary condition for prediction to be valid i.e. else the task is not of prediction but of pattern recognition (a subtle but important distinction).  

I run this below:

```{r}
a2 <- Gls(twstrs ~ treat * rcs(week, c(3,6,10)) + rcs(twstrs0, 3) + rcs(age, 4) * sex, 
         data = both_train1,
         correlation = corCAR1(form = ~ week | id), 
         na.action = na.omit)

modDat1_dt <- both_train1 |> 
  na.omit() |> 
  mutate(yHat = as.numeric(fitted(a2))) |> 
  rowid_to_column() |> 
  rename(y = twstrs) |> 
  data.table::as.data.table()
# id is now rowid:
modDat1_dt2 <- response_vec(y = modDat1_dt$y, yhat = modDat1_dt$yHat, id = modDat1_dt$rowid, nEnv = NA) |> rename(rowid = id)

modDat1_dt2[modDat1_dt[,c("rowid","id","treat","age","sex","twstrs0","week")], on = .(rowid)] |> filter(id == 11) |> with({ # sample(1:99,1) # a few large error cases 95,53(this one looks weird in terms of test value),100,11,84
  # browser()
  out <- both_test1 |> rename(ID = id) |> filter(ID == unique(id))
  
  # modDatEx1_yEnv_c <- Gls(yEnv ~ rcs(week, c(3,6,10)))
  # modDatEx1_yEnv_c <- Gls(yEnv ~ rcs(week, c(2,4,8,12)))
  ## modDatEx1_yEnv_c <- Gls(yEnv ~ rcs(week, c(3:6)))
  # modDatEx1_yEnv_c <- Gls(yEnv ~ rcs(week, c(3,6,9,12)))
  ## modDatEx1_yEnv_c_preds <- predict(modDatEx1_yEnv_c, newdata = out)
    
  # smoothing splines:
  # modDatEx1_yEnv_c <- stats::smooth.spline(x = week, y = yEnv, cv = FALSE, tol = 0.00001) # GCV
  modDatEx1_yEnv_c <- stats::smooth.spline(x = week, y = yEnv, df = 2, tol = 0.00001)
  # out-of-sample predictions:
  modDatEx1_yEnv_c_preds <- predict(modDatEx1_yEnv_c, x = out$week)$y
  
  # modDatEx1_yEnv_c <- Gls(yEnv ~ week)
  # using rcs:
  # modDatEx1_yEnv_c <- Gls(yEnv ~ rcs(week, c(3,6,10)))
  # modDatEx1_yEnv_c_preds <- predict(modDatEx1_yEnv_c, newdata = out)
  
  plot(c(week, out$week), c(yEnv,out$twstrs), type = "p", ylim = c(min(c(y,yEnv,out$twstrs,modDatEx1_yEnv_c_preds)), max(c(y,yEnv,out$twstrs,modDatEx1_yEnv_c_preds))), xlab = "time (sec)", ylab = "yEnv", xaxt = "n", yaxt = "n", col = scales::alpha("steelblue", 0.3), bg = scales::alpha("steelblue", 0.3), pch = 21) # alpha 0.008 for more data
  x_ticks <- axis(1, at = seq(min(c(week, out$week)), max(c(week, out$week)), by = round(range(c(week, out$week))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(c(yEnv, out$twstrs, modDatEx1_yEnv_c_preds)), max(c(yEnv, out$twstrs, modDatEx1_yEnv_c_preds)), by = round(range(c(yEnv, out$twstrs, modDatEx1_yEnv_c_preds))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  points(out$week, out$twstrs, col = scales::alpha("black", 0.3), bg = scales::alpha("black", 0.3), pch = 21)
  points(unique(week), y[cumsum(table(week))], col = scales::alpha("darkorange", 1), bg = scales::alpha("darkorange", 1), pch = 21)
  lines(week, as.numeric(fitted(modDatEx1_yEnv_c)), col = "darkorange", lwd = 3)
  points(out$week, modDatEx1_yEnv_c_preds, col = "darkgreen", lwd = 3)
  title(paste0("ID - ", id[1]))
  })

# using flex spline:
modDat1_dt2[modDat1_dt[,c("rowid","id","treat","age","sex","twstrs0","week")], on = .(rowid)]  |> group_by(id) |> 
  filter(n_distinct(week) >= 4) |> # stats::smooth.spline() needs at least 4 unique 'x' values
  nest() |> 
  mutate(abs_error = map(data, function(df) {
  idx <- cur_group()$id
  out <- both_test1 |> filter(id == idx)
  modDatEx1_yEnv_c <- stats::smooth.spline(x = data[[1]]$week, y = data[[1]]$yEnv, cv = FALSE, tol = 0.00001)
  modDatEx1_yEnv_c_preds <- predict(modDatEx1_yEnv_c, x = out$week)$y
  abs(out$twstrs-modDatEx1_yEnv_c_preds[[1]])
})) |> select(-data) |> unnest(abs_error) |> 
  ungroup() |> # arrange(-abs_error) # 53 seems weird large error case
  na.omit() |> # 1 NA case with id 49 # mae is 5.61
  filter(!(id %in% c(53))) |>
  summarise(mae = mean(abs_error))
# 5.64

# using linear spline:
modDat1_dt2[modDat1_dt[,c("rowid","id","treat","age","sex","twstrs0","week")], on = .(rowid)]  |> group_by(id) |> 
  filter(n_distinct(week) >= 4) |> # stats::smooth.spline() needs at least 4 unique 'x' values
  nest() |> 
  mutate(abs_error = map(data, function(df) {
  idx <- cur_group()$id
  out <- both_test1 |> filter(id == idx)
  modDatEx1_yEnv_c <- stats::smooth.spline(x = data[[1]]$week, y = data[[1]]$yEnv, df = 2, tol = 0.00001)
  modDatEx1_yEnv_c_preds <- predict(modDatEx1_yEnv_c, x = out$week)$y
  abs(out$twstrs-modDatEx1_yEnv_c_preds[[1]])
})) |> select(-data) |> unnest(abs_error) |> 
  ungroup() |> # arrange(-abs_error) # 53 seems weird large error case
  na.omit() |> # 1 NA case with id 49 # mae is 5.26
  filter(!(id %in% c(53))) |> 
  # pull(id) -> idsKept
  summarise(mae = mean(abs_error))
# 4.91

# normal prediction using a2 model:
both_test1 |> mutate(preds_y = as.numeric(predict(a2, newdata = as.data.table(both_test1 |> select(id,treat,age,sex,twstrs0,week))))) |> 
  # filter(is.na(twstrs)) |> pull(id) # c(4, 49, 65, 85)
  filter(id %in% idsKept) |> 
  summarise(err_y = mean(abs(twstrs-preds_y)))
# 6.17
# using linear spline this is a 
(6.17-4.91)/6.17 
# 20.42% error reduction due to the yenv procedure
# # using flex spline is not as effective as it gives just a 
(6.17-5.64)/6.17 
# 8.5% error reduction. Is this because the starting a2 model is already using splines? I think it could be due to the small data size and not so much due to the starting spline model. 
rm(idsKept)
```


# Prediction Error Adventure - Part 1

```{r}
# True signal:
x <- seq(0, 100, by = 0.2)
y <- (1.02)^x 
plot(x, y) # exponential growth with x
# Observe a sample subset contaminated with noise:
set.seed(023)
xSubIdx <- which(x %in% sample(x, size = 100))
xSub <- x[xSubIdx]
ySub <- y[xSubIdx] + rnorm(100, mean = 0, sd = 1)
plot(xSub, ySub) # Can't discern the true pattern due to noise.
# Split data in a train set and two types of test sets: One is extending beyond the x limits of the train set, and one is interior to those limits (note the if x is time then in real scenarios we cannot have the interior type i.e. can't travel backwards in time).
trainX <- xSub[1:80]
testXout <- xSub[81:100]
testXin <- sample(x[x<80], size = length(testXout)) # Picking the interior test set from population signal.
testXinIdx <- which(x %in% testXin) # Storing the interior x indices.
trainY <- ySub[1:80]
testYout <- ySub[81:100]
testYin <- y[testXinIdx] # Picking the interior test set from population signal.
# Model on the train set. Note here the model is misspecified cos we don't know the true exponential pattern:
m <- lm(y ~ x, data = data.frame(x = trainX, y = trainY))
summary(m) # Fit is excellent even though the model is misspecified!

plot(c(trainX, testXout), c(trainY, testYout))
abline(reg = m)
points(testXin, testYin, col = "blue", pch = 16)
points(testXout, testYout, col = "red", pch = 16)
# Interior MSE:
mseInt <- mean((testYin - predict(m, newdata = data.frame(x = testXin)))^2)
# Exterior MSE:
mseExt <- mean((testYout - predict(m, newdata = data.frame(x = testXout)))^2)
# Here mseInt < mseExt

# function:
library(tidyverse)

simPredError <- function(mod = c("lm","nls"), xTrue = x, yTrue = (1.05)^x, sampleSize = round(0.2*length(x)), stnr = 10, xTime = FALSE, plot = TRUE) {
  # sampleSize: The sample we observe i.e. subject to noise
  # stnr: signal to noise ratio i.e. signal is 10 times more than noise 
  # browser()
  mod_type <- match.arg(mod)
  x <- seq(0, 100, by = 0.2)
  xSubIdx <- which(xTrue %in% sample(xTrue, size = sampleSize))
  xSub <- xTrue[xSubIdx]
  ySub <- yTrue[xSubIdx]
  sdNoise <- sapply(ySub, function(x) rnorm(n = 1, mean = x/stnr, sd = 0.01)) # check
  # summary(ySub/sdNoise)
  ySubNoise <- sapply(sdNoise, function(x) rnorm(n = 1, mean = 0, sd = x))
  # update noise contaminated signal:
  ySub <- ySub + ySubNoise 
  # Was:
  # ySub <- yTrue[xSubIdx] + rnorm(n = length(xSubIdx), mean = 0, sd = sdNoise)
  # where sdNoise was i.e. 10, 20 etc
  
  trainIdx <- round(0.8*max(xTrue)) # Can also represent an x variable value.
  trainX <- xSub[xSub <= trainIdx] # was trainX <- xSub[1:trainIdx]
  testXout <- xSub[xSub > trainIdx] # was testXout <- xSub[(trainIdx+1):max(xTrue)]
  trainY <- ySub[xSub <= trainIdx] # was trainY <- ySub[1:trainIdx]
  
  if (!xTime) {
    # xNew <- xTrue[!(xTrue %in% xSub)] # Making sure that all cases are new compared to train set.
    # testXin <- sample(xNew[xNew<trainIdx], size = length(testXout))
    # testXinIdx <- which(xTrue %in% testXin)
    # testYin <- yTrue[testXinIdx] # Also needs to be with noise. 
    
    testXin <- sort(sample(trainX, size = length(testXout)))
    testXinIdx <- which(trainX %in% testXin)
    testYin <- trainY[testXinIdx]
    trainX <- trainX[-testXinIdx]
    trainY <- trainY[-testXinIdx]
    
  } else { # Need to recheck:
    testXin <- sort(sample(trainX, size = length(testXout))) # Case where x represents time i.e. can only use previously seen time records
    testXinIdx <- which(trainX %in% testXin)
    testYin <- trainY[testXinIdx]
  }
  
  testYout <- ySub[xSub > trainIdx] # was testYout <- ySub[(trainIdx+1):length(xSub)]
  
  if (mod_type == "lm") {
    # Model misspecification:
    m <- lm(y ~ x, data = data.frame(x = trainX, y = trainY))
    # summary(m_lm)
  } else {
   # Model correctness:
    m <- tryCatch(
      stats::nls(y ~ a^x, data = data.frame(x = trainX, y = trainY), trace = TRUE, 
               start = list(a = 1)), error = function(e) {NULL} 
    )
    # 
    # summary(m_nls) 
  }
  
  
  if (plot & !is.null(m)) {
    xPlot <- round(c(trainX, testXout), digits = 2)
    yPlot <- round(c(trainY, testYout), digits = 2)
    plot(xPlot, yPlot, xlab = ifelse(!xTime, "X", "Time"), ylab = "Y", xaxt = "n", yaxt = "n", col = "white", ylim = c(min(yPlot), max(yPlot)))
    x_ticks <- axis(1, at = xPlot, las = 2, cex.axis = 0.5)
    # x_ticks <- axis(1, at = xPlot, las = 2, cex.axis = 0.7, labels = FALSE)
    # text(x = xPlot, y = par("usr")[3] - 5, labels = xPlot, xpd = NA, srt = 35, cex = 0.7, adj = 0.2)
    y_ticks <- axis(2, at = seq(min(yPlot), max(yPlot), by = round(range(yPlot)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
    abline(v = x_ticks, h = y_ticks, lwd = 0.7, lty = 3, col = "lightgray")
    points(xPlot, yPlot, col = "steelblue", cex = 0.75, pch = 16)
    lines(trainX, predict(m), col = "cyan", lwd = 2)
    points(testXin, testYin, col = "blue", cex = 0.75, pch = 16)
    points(testXout, testYout, col = "brown", cex = 0.75, pch = 16)
    legend("bottomright", c("Actual", ifelse(mod_type == "lm", "linear", "non-linear"), "Internal", "External"), cex = 0.7, col = c("steelblue", "cyan", "blue", "brown"), pch = c(16,16,16,16), inset = c(0.03, 1), xpd = TRUE, horiz = TRUE, bty = "n", text.width = c(10, 10, 10,10))
  }
  # # Interior MSE:
  # if (!xTime) {
  #   mseInt <- mean((testYin - predict(m, newdata = data.frame(x = testXin)))^2)
  # } else { # recheck i.e. no change now:
  #   # When x is time then the only backwards option for the MSE is the estimate from the fitted model adjusted by the length of interior test data i.e. time cannot be revisited:
  #   # MSE on all training data:
  #   mseIntAll <- mean((trainY - predict(m, newdata = data.frame(x = trainX)))^2)
  #   # MSE on training data subset but then division by subset size to total size
  #   mseInt <- mseIntAll *  length(testXin) / length(trainX)
  # }
  
  # check that predictions fall in fitted line/curve:
  # points(testXin, predict(m, newdata = data.frame(x = testXin)), col = "green")
  
  if (!is.null(m)) {
  # Interior MSE:
  mseInt <- mean((testYin - predict(m, newdata = data.frame(x = testXin)))^2)
  # Exterior MSE:
  mseExt <- mean((testYout - predict(m, newdata = data.frame(x = testXout)))^2)
  } else {
    mseInt <- mseExt <- NA
  } 

  return(tibble(mseInt = mseInt, mseExt = mseExt))
}

### With misspecified model:
# checks:
simPredError()

simPredErrorRes <- map(1:10, ~ simPredError()) |> list_rbind()
#
simPredErrorRes <- map(1:1e3, ~ simPredError(plot = FALSE)) |> list_rbind()

plot(1:1e3, simPredErrorRes$mseExt, col = "red", type = "l")
lines(1:1e3, simPredErrorRes$mseInt, col = "blue", type = "l")
abline(h = mean(simPredErrorRes$mseExt), col = "red", lwd = 2)
abline(h = mean(simPredErrorRes$mseInt), col = "blue", lwd = 2)

# Sample size increase (double):
simPredError(sampleSize = 200)

simPredErrorRes <- map(1:1e3, ~ simPredError(plot = FALSE, sampleSize = 200)) |> list_rbind()

plot(1:1e3, simPredErrorRes$mseExt, col = "red", type = "l")
lines(1:1e3, simPredErrorRes$mseInt, col = "blue", type = "l")
abline(h = mean(simPredErrorRes$mseExt), col = "red", lwd = 2)
abline(h = mean(simPredErrorRes$mseInt), col = "blue", lwd = 2)

# Noise increase:
simPredError(stnr = 3)
simPredErrorRes <- map(1:1e3, ~ simPredError(plot = FALSE, sdNoise = 2)) |> list_rbind()
simPredErrorRes |> summarise(across(1:2, mean))
#   mseInt mseExt
#    <dbl>  <dbl>
# 1   2.38   6.08

plot(1:1e3, simPredErrorRes$mseExt, col = "red", type = "l")
lines(1:1e3, simPredErrorRes$mseInt, col = "blue", type = "l")
abline(h = mean(simPredErrorRes$mseExt), col = "red", lwd = 2)
abline(h = mean(simPredErrorRes$mseInt), col = "blue", lwd = 2)

# when x is time and noise increase:
simPredErrorRes <- map(1:1e3, ~ simPredError(plot = FALSE, xTime = TRUE, sdNoise = 2)) |> list_rbind()
simPredErrorRes |> summarise(across(1:2, mean))
#   mseInt mseExt
#    <dbl>  <dbl>
# 1   0.99   5.89

plot(1:1e3, simPredErrorRes$mseExt, col = "red", type = "l")
lines(1:1e3, simPredErrorRes$mseInt, col = "blue", type = "l")
abline(h = mean(simPredErrorRes$mseExt), col = "red", lwd = 2)
abline(h = mean(simPredErrorRes$mseInt), col = "blue", lwd = 2)
# red lines are completely separated from the blue ones i.e. higher errors compared to a non-time x

# when x is time and noise increase and double sample size:
simPredErrorRes <- map(1:1e3, ~ simPredError(plot = FALSE, xTime = TRUE, sdNoise = 2, sampleSize = 200)) |> list_rbind()
simPredErrorRes |> summarise(across(1:2, mean))
# 1.00   5.79

plot(1:1e3, simPredErrorRes$mseExt, col = "red", type = "l", ylim = c(0,13))
lines(1:1e3, simPredErrorRes$mseInt, col = "blue", type = "l")
abline(h = mean(simPredErrorRes$mseExt), col = "red", lwd = 2)
abline(h = mean(simPredErrorRes$mseInt), col = "blue", lwd = 2)
# red lines are even more completely separated from the blue ones i.e. higher errors compared to a non-time x

### With model correctness:
simPredErrorRes <- map(1:1e3, ~ simPredError(mod = "nls", plot = FALSE)) |> list_rbind()

plot(1:1e3, simPredErrorRes$mseExt, col = "red", type = "l")
lines(1:1e3, simPredErrorRes$mseInt, col = "blue", type = "l")
abline(h = mean(simPredErrorRes$mseExt), col = "red", lwd = 2)
abline(h = mean(simPredErrorRes$mseInt), col = "blue", lwd = 2)
# reverse situation i.e. model becomes appropriate after the 80 mark

# Sample size increase (double):
simPredErrorRes <- map(1:1e3, ~ simPredError(mod = "nls", plot = FALSE, sampleSize = 200)) |> list_rbind()

plot(1:1e3, simPredErrorRes$mseExt, col = "red", type = "l")
lines(1:1e3, simPredErrorRes$mseInt, col = "blue", type = "l")
abline(h = mean(simPredErrorRes$mseExt), col = "red", lwd = 2)
abline(h = mean(simPredErrorRes$mseInt), col = "blue", lwd = 2)

# Noise increase:
simPredErrorRes <- map(1:1e3, ~ simPredError(mod = "nls", plot = FALSE, sdNoise = 2)) |> list_rbind()
simPredErrorRes |> summarise(across(1:2, mean))
#   mseInt mseExt
#    <dbl>  <dbl>
# 1   2.37  4.64

plot(1:1e3, simPredErrorRes$mseExt, col = "red", type = "l")
lines(1:1e3, simPredErrorRes$mseInt, col = "blue", type = "l")
abline(h = mean(simPredErrorRes$mseExt), col = "red", lwd = 2)
abline(h = mean(simPredErrorRes$mseInt), col = "blue", lwd = 2)
# red lines are more or less separated from the blue ones

# when x is time and noise increase:
simPredErrorRes <- map(1:1e3, ~ simPredError(mod = "nls", plot = FALSE, xTime = TRUE, sdNoise = 2)) |> list_rbind()
simPredErrorRes |> summarise(across(1:2, mean))
#   mseInt mseExt
#    <dbl>  <dbl>
# 1   0.99   4.69

plot(1:1e3, simPredErrorRes$mseExt, col = "red", type = "l")
lines(1:1e3, simPredErrorRes$mseInt, col = "blue", type = "l")
abline(h = mean(simPredErrorRes$mseExt), col = "red", lwd = 2)
abline(h = mean(simPredErrorRes$mseInt), col = "blue", lwd = 2)
# red lines are completely separated from the blue ones i.e. higher errors compared to a non-time x

library(furrr)
plan(multisession, workers = 8)

# checks with simpler example:
# names should be similar to function argument names:
# conditionsGrid <- expand_grid(
#   mod =  c("lm", "nls"),
#   sampleSize = c(50,150,300),
#   stnr = c(0.5,3,7),
#   xTime = FALSE
# )
# pmap(conditionsGrid, function(mod_i, sampleSize_i, stnr_i, xTime_i) {
#   x <- seq(0, 100, by = 0.2)
#        map(1:3, ~ simPredError(mod = mod_i, 
#                       xTrue = x, 
#                       yTrue = (1.05)^x, 
#                       sampleSize = sampleSize_i, 
#                       stnr = stnr_i, 
#                       xTime = xTime_i, 
#                       plot = FALSE)) |> 
#          list_rbind() |> summarise(across(1:2, mean)) 
# })

conditionsGrid <- expand_grid(
  mod =  c("lm", "nls"),
  sampleSize = seq(100, 400, 25), # took out 50 due to nls convergence issues
  stnr = seq(0.6, 3, 0.3), # took out 0.3 due to nls convergence issues
  xTime = c(FALSE, TRUE)
)

tictoc::tic()
simPredErrorRes <- future_pmap(conditionsGrid, function(mod_i, sampleSize_i, stnr_i, xTime_i) {
  # this is needed here for more flexibility i.e. simPredError would also run by completely leaving out the xTrue and yTrue arguments below:
  x <- seq(0, 100, by = 0.2)
  # using ~ or function(i) {} would also work
  map(1:3e2, ~ simPredError(mod = mod_i, 
                          xTrue = x, 
                          yTrue = (1.05)^x, 
                          sampleSize = sampleSize_i, 
                          stnr = stnr_i, 
                          xTime = xTime_i, 
                          plot = FALSE)) |> 
    list_rbind() |> 
    summarise(across(1:2, mean))  
}, 
.options = furrr_options(seed = TRUE)) |> 
  list_rbind() |> 
  bind_cols(conditionsGrid)
tictoc::toc()
# 300 secs

conflicted::conflict_scout()
# 2 conflicts
# • `filter()`: dplyr and stats
# • `lag()`: dplyr and stats

simPredErrorRes |> 
  dplyr::filter(xTime == FALSE) |> 
  ggplot(aes(mseInt, mseExt, col = mod)) + 
  geom_point() +
  geom_smooth(se = FALSE) + 
  theme_linedraw()

simPredErrorRes |> 
  dplyr::filter(xTime == FALSE) |> 
  ggplot(aes(mseInt, col = mod)) + 
  geom_density() +
  theme_linedraw()

simPredErrorRes |> 
  dplyr::filter(xTime == FALSE) |> 
  ggplot(aes(mseExt, col = mod)) + 
  geom_density() +
  theme_linedraw()

simPredErrorRes |> pivot_longer(cols = 1:2, names_to = "errorType", values_to = "avgValue") |>
  dplyr::filter(mod == "lm", xTime == FALSE) |> 
  ggplot(aes(sampleSize, avgValue, col = errorType)) + 
  geom_point() +
  geom_smooth(se = FALSE, span = 1) + 
  theme_linedraw()

simPredErrorRes |> pivot_longer(cols = 1:2, names_to = "errorType", values_to = "avgValue") |>
  dplyr::filter(mod == "lm", xTime == FALSE) |> 
  ggplot(aes(stnr, avgValue, col = errorType)) + 
  geom_point() +
  geom_smooth(se = FALSE, span = 1) + 
  theme_linedraw()

simPredErrorRes |> pivot_longer(cols = 1:2, names_to = "errorType", values_to = "avgValue") |>
  dplyr::filter(errorType == "mseInt", xTime == FALSE) |> 
  ggplot(aes(sampleSize, avgValue, col = mod)) + 
  geom_point() +
  geom_smooth(se = FALSE, span = 1) + 
  theme_linedraw()

simPredErrorRes |> pivot_longer(cols = 1:2, names_to = "errorType", values_to = "avgValue") |>
  dplyr::filter(errorType == "mseExt", xTime == FALSE) |> 
  ggplot(aes(sampleSize, avgValue, col = mod)) + 
  geom_point() +
  geom_smooth(se = FALSE, span = 1) + 
  theme_linedraw()

simPredErrorRes |> pivot_longer(cols = 1:2, names_to = "errorType", values_to = "avgValue") |>
  dplyr::filter(errorType == "mseInt", xTime == FALSE) |> 
  ggplot(aes(stnr, avgValue, col = mod)) + 
  geom_point() +
  geom_smooth(se = FALSE, span = 1) + 
  theme_linedraw()

simPredErrorRes |> pivot_longer(cols = 1:2, names_to = "errorType", values_to = "avgValue") |>
  dplyr::filter(errorType == "mseExt", xTime == FALSE) |> 
  ggplot(aes(stnr, avgValue, col = mod)) + 
  geom_point() +
  geom_smooth(se = FALSE, span = 1) + 
  theme_linedraw()

simPredErrorRes |> 
  group_by(mod) |> 
  summarise(avgMSEint = mean(mseInt), avgMSEext = mean(mseExt))

# different grid:
conditionsGrid2 <- expand_grid(
  mod =  c("lm", "nls"),
  sampleSize = seq(50, 450, 25), 
  stnr = seq(0.1, 10, 0.3), 
  xTime = c(FALSE, TRUE)
)

tictoc::tic()
simPredErrorRes2 <- future_pmap(conditionsGrid2, function(mod_i, sampleSize_i, stnr_i, xTime_i) {
  # this is needed here for more flexibility i.e. simPredError would also run by completely leaving out the xTrue and yTrue arguments below:
  x <- seq(0, 100, by = 0.2)
  # using ~ or function(i) {} would also work
  map(1:3e2, ~ simPredError(mod = mod_i, 
                          xTrue = x, 
                          yTrue = (1.05)^x, 
                          sampleSize = sampleSize_i, 
                          stnr = stnr_i, 
                          xTime = xTime_i, 
                          plot = FALSE)) |> 
    list_rbind() |> 
    summarise(across(1:2, mean))  
}, 
.options = furrr_options(seed = TRUE)) |> 
  list_rbind() |> 
  bind_cols(conditionsGrid2)
tictoc::toc()
# 23 min


write_csv(simPredErrorRes2 |> 
            mutate(across(where(is.double), ~ round(.x, digits = 2))) |> 
            mutate(mod = ifelse(mod == "lm", 0, 1), xTime = ifelse(xTime == FALSE, 0, 1)) |> 
            rownames_to_column(var = "ID") |> 
            mutate(ID = paste0("sim", ID)) |> 
            relocate(mod, .after = ID) |> 
            rename(treatment = mod) |> 
            na.omit(), "simPredErrorRes2.csv")

simPredErrorRes2 <- simPredErrorRes2 |> na.omit()

with(data = simPredErrorRes2, { # lm
  plot(mseInt, mseExt)
  points(mseInt[mod == "lm" & stnr == 0.1], mseExt[mod == "lm" & stnr == 0.1], col = "red")
  points(mseInt[mod == "lm" & stnr == 0.4], mseExt[mod == "lm" & stnr == 0.4], col = "purple")
  points(mseInt[mod == "lm" & stnr %in% c(0.7,1)], mseExt[mod == "lm" & stnr %in% c(0.7,1)], col = "orange")
  points(mseInt[mod == "lm" & stnr > 1], mseExt[mod == "lm" & stnr > 1], col = "magenta")
})

### Better plots:
stnr_list <- round(unique(conditionsGrid2$stnr), digits = 1)
# mseInt
simPredErrorRes2 |> filter(stnr == stnr_list[10]) |> with({ 
  # with stnr_list[1] there's only lm values due to nls NAs that have been removed 
  plot(sampleSize, mseInt, type = "p", ylim = c(min(mseInt), max(mseInt)), xlab = "sample size", ylab = "Internal MSE", xaxt = "n", yaxt = "n", col = "white")
  x_ticks <- axis(1, at = sampleSize, las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(mseInt), max(mseInt), by = round(range(mseInt)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  abline(v = x_ticks, h = y_ticks, lwd = 0.7, lty = 3, col = "lightgray")
  points(sampleSize[mod == "lm"], mseInt[mod == "lm"], col = scales::alpha("firebrick1", 0.5), bg = scales::alpha("firebrick1", 0.5), pch = 21)
  points(sampleSize[mod == "nls"], mseInt[mod == "nls"], col = scales::alpha("darkgreen", 0.5), bg = scales::alpha("darkgreen", 0.5), pch = 21)
  legend("bottomright", c("lm (misspecified)", "nls (correct)"), cex = 1, col = c("firebrick1", "darkgreen"), pch = c(16,16), inset = c(0.03, 0.95), xpd = TRUE, horiz = TRUE, bty = "n", text.width = c(60, 30))
  smFit <- stats::smooth.spline(sampleSize[mod == "lm"], mseInt[mod == "lm"], df = 3)
  lines(smFit$x, smFit$y, col = "firebrick1", lwd = 3)
  smFit <- stats::smooth.spline(sampleSize[mod == "nls"], mseInt[mod == "nls"], df = 3)
  lines(smFit$x, smFit$y, col = "darkgreen", lwd = 3)
})
# sample size does not seem to play a role and from stnr_list[8] (2.2) complete separation starts and stays the same with sample size. As stnr gets larger the separation becomes constant. As stnr increases error goes down for both models.

# error difference between models:
simPredErrorRes2 |> filter(stnr == stnr_list[25]) |> with({ 
  # with stnr_list[1] there's only lm values due to nls NAs that have been removed 
  mod1ErrBySize <- as.numeric(by(mseInt[mod == "lm"], sampleSize[mod == "lm"], mean))
  mod2ErrBySize <- as.numeric(by(mseInt[mod == "nls"], sampleSize[mod == "nls"], mean))
  diff <- abs(mod1ErrBySize-mod2ErrBySize)
  plot(unique(sampleSize), diff, xlab = "sample size", ylab = "Internal MSE", xaxt = "n", yaxt = "n", col = "white")
  x_ticks <- axis(1, at = unique(sampleSize), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(diff), max(diff), by = round(range(diff)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  abline(v = x_ticks, h = y_ticks, lwd = 0.7, lty = 3, col = "lightgray")
  points(unique(sampleSize), diff, col = "magenta", bg = "magenta", pch = 21)
  smFit <- stats::smooth.spline(unique(sampleSize), diff, df = 3)
  lines(smFit$x, smFit$y, col = "magenta", lwd = 3)
  legend("bottomright", c("absolute difference (lm vs nls)"), cex = 1, col = "magenta", pch = 16, inset = c(0.03, 0.95), xpd = TRUE, horiz = TRUE, bty = "n", text.width = 90)
})

simPredErrorRes2 |> with({ 
  # with stnr_list[1] there's only lm values due to nls NAs that have been removed 
  mod1ErrBySize <- as.numeric(by(mseInt[mod == "lm"], sampleSize[mod == "lm"], mean))
  mod2ErrBySize <- as.numeric(by(mseInt[mod == "nls"], sampleSize[mod == "nls"], mean))
  diff <- abs(mod1ErrBySize-mod2ErrBySize)
  plot(unique(sampleSize), diff, xlab = "sample size", ylab = "Internal MSE", xaxt = "n", yaxt = "n", col = "white")
  x_ticks <- axis(1, at = unique(sampleSize), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(diff), max(diff), by = round(range(diff)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  abline(v = x_ticks, h = y_ticks, lwd = 0.7, lty = 3, col = "lightgray")
  points(unique(sampleSize), diff, col = "magenta", bg = "magenta", pch = 21)
  smFit <- stats::smooth.spline(unique(sampleSize), diff, df = 3)
  lines(smFit$x, smFit$y, col = "magenta", lwd = 3)
  legend("bottomright", c("absolute difference (lm vs nls)"), cex = 1, col = "magenta", pch = 16, inset = c(0.03, 0.95), xpd = TRUE, horiz = TRUE, bty = "n", text.width = 90)
})

# mseExt
simPredErrorRes2 |> filter(stnr == stnr_list[7]) |> with({ 
  # with stnr_list[1] there's only lm values due to nls NAs that have been removed 
  plot(sampleSize, mseExt, type = "p", ylim = c(min(mseExt), max(mseExt)), xlab = "sample size", ylab = "External MSE", xaxt = "n", yaxt = "n", col = "white")
  x_ticks <- axis(1, at = sampleSize, las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(mseExt), max(mseExt), by = round(range(mseExt)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  abline(v = x_ticks, h = y_ticks, lwd = 0.7, lty = 3, col = "lightgray")
  points(sampleSize[mod == "lm"], mseExt[mod == "lm"], col = scales::alpha("firebrick1", 0.5), bg = scales::alpha("firebrick1", 0.5), pch = 21)
  points(sampleSize[mod == "nls"], mseExt[mod == "nls"], col = scales::alpha("darkgreen", 0.5), bg = scales::alpha("darkgreen", 0.5), pch = 21)
  legend("bottomright", c("lm (misspecified)", "nls (correct)"), cex = 1, col = c("firebrick1", "darkgreen"), pch = c(16,16), inset = c(0.03, 0.95), xpd = TRUE, horiz = TRUE, bty = "n", text.width = c(60, 30))
  # browser()
  smFit <- stats::smooth.spline(sampleSize[mod == "lm"], mseExt[mod == "lm"], df = 3)
  lines(smFit$x, smFit$y, col = "firebrick1", lwd = 3)
  smFit <- stats::smooth.spline(sampleSize[mod == "nls"], mseExt[mod == "nls"], df = 3)
  lines(smFit$x, smFit$y, col = "darkgreen", lwd = 3)
})
# sample size plays a role from stnr_list[3] (0.7) when separation starts and increases with sample size, but as stnr gets larger the separation becomes constant and independent of sample size. Complete separation from stnr_list[4] (1). As stnr increases error goes down for both models.
# Correct model error continues downtrend per sample size even until stnr_list[17]. 

# error difference between models:
simPredErrorRes2 |> filter(stnr == stnr_list[20]) |> with({ 
  # with stnr_list[1] there's only lm values due to nls NAs that have been removed 
  mod1ErrBySize <- as.numeric(by(mseExt[mod == "lm"], sampleSize[mod == "lm"], mean))
  mod2ErrBySize <- as.numeric(by(mseExt[mod == "nls"], sampleSize[mod == "nls"], mean))
  diff <- abs(mod1ErrBySize-mod2ErrBySize)
  plot(unique(sampleSize), diff, xlab = "sample size", ylab = "External MSE", xaxt = "n", yaxt = "n", col = "white")
  x_ticks <- axis(1, at = unique(sampleSize), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(diff), max(diff), by = round(range(diff)[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  abline(v = x_ticks, h = y_ticks, lwd = 0.7, lty = 3, col = "lightgray")
  points(unique(sampleSize), diff, col = "magenta", bg = "magenta", pch = 21)
  smFit <- stats::smooth.spline(unique(sampleSize), diff, df = 3)
  lines(smFit$x, smFit$y, col = "magenta", lwd = 3)
  legend("bottomright", c("absolute difference (lm vs nls)"), cex = 1, col = "magenta", pch = 16, inset = c(0.03, 0.95), xpd = TRUE, horiz = TRUE, bty = "n", text.width = 90)
})

simPredErrorRes2 |> with({ 
  # with stnr_list[1] there's only lm values due to nls NAs that have been removed 
  # browser()
  mod1ErrBySize <- as.numeric(by(mseExt[mod == "lm" & stnr >= 0.4], tibble(sampleSize = sampleSize[mod == "lm" & stnr >= 0.4], stnr = stnr[mod == "lm" & stnr >= 0.4]), mean))
  mod2ErrBySize <- as.numeric(by(mseExt[mod == "nls" & stnr >= 0.4], tibble(sampleSize = sampleSize[mod == "nls" & stnr >= 0.4], stnr = stnr[mod == "nls" & stnr >= 0.4]), mean))
  size <- as.numeric(by(sampleSize[mod == "lm" & stnr >= 0.4], tibble(sampleSize = sampleSize[mod == "lm" & stnr >= 0.4], stnr = stnr[mod == "lm" & stnr >= 0.4]), mean))
  diff <- abs(mod1ErrBySize-mod2ErrBySize) # |> stats::na.omit()
  plot(size, diff, xlab = "sample size", ylab = "External MSE", xaxt = "n", yaxt = "n", col = "white")
  x_ticks <- axis(1, at = unique(size), las = 2, cex.axis = 0.5)
  y_ticks <- axis(2, at = seq(min(stats::na.omit(diff)), max(stats::na.omit(diff)), by = round(range(stats::na.omit(diff))[2]/1e2, digits = 2)), las = 2, cex.axis = 0.5)
  abline(v = x_ticks, h = y_ticks, lwd = 0.7, lty = 3, col = "lightgray")
  points(size, diff, col = "magenta", bg = "magenta", pch = 21)
  smFit <- stats::smooth.spline(size[!is.na(diff)], stats::na.omit(diff))
  lines(smFit$x, smFit$y, col = "magenta", lwd = 3)
  legend("bottomright", c("absolute difference (lm vs nls)"), cex = 1, col = "magenta", pch = 16, inset = c(0.03, 0.95), xpd = TRUE, horiz = TRUE, bty = "n", text.width = 90)
})




# remotes::install_github("trelliscope/trelliscope")
library(trelliscope)
d <- simPredErrorRes2 |> 
  mutate(across(where(is.double), ~ round(.x, digits = 2))) |> 
  rownames_to_column(var = "ID") |> 
  mutate(ID = paste0("sim", ID)) |> 
  na.omit()
d
panel_dat <- (
  ggplot(d, aes(mseInt, mseExt)) +
    geom_line() + 
    theme_linedraw() + 
    facet_panels(vars(mod, xTime))
  ) |>
  as_panels_df(as_plotly = TRUE) |> 
  as_trelliscope_df(name = "prediction_error_types")

view_trelliscope(panel_dat)

panel_dat <- (
  ggplot(d, aes(sampleSize, mseInt)) +
    geom_line() + 
    theme_linedraw() + 
    facet_panels(scales = "free", vars(mod, xTime))
  ) |>
  as_panels_df(as_plotly = TRUE) |> 
  as_trelliscope_df(name = "prediction_error_types")

view_trelliscope(panel_dat)

panel_dat <- (
  ggplot(d, aes(stnr, mseInt)) +
    geom_line() + 
    theme_linedraw() + 
    facet_panels(scales = "free", vars(mod, xTime))
  ) |>
  as_panels_df(as_plotly = TRUE) |> 
  as_trelliscope_df(name = "prediction_error_types")

view_trelliscope(panel_dat)
```


# References 

DSP First, A Multimedia Approach McClellan et al. 1998

- Evaluation of clinical prediction models (part 1): from development to external validation (Harell):
https://www.bmj.com/content/384/bmj-2023-074819.full

https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118263

https://www.atsjournals.org/doi/10.1513/AnnalsATS.201612-997FR#:~:text=Ramp%20protocols%20are%20characterized%20by,1%20W%20every%206%20seconds.

https://journals.plos.org/plosone/

https://www.mayoclinic.org/healthy-lifestyle/fitness/in-depth/exercise-intensity/art-20046887

https://physiofitness.org.uk/what-should-our-maximum-heart-rate-be-during-exercise/



# Notes 

```{r}
# For interactive usage, auto_browse() is useful because it automatically starts a browser() in the right place:
?purrr::auto_browse
```

See page 80 for recovering the envelope from the center signal. 

Offers an explanation of the effect of subsampling methods like bootstrapping, observation weighting and under/over sampling. 

See quora bookmarks for potentially related topic "Does it make sense to use the outcomes of a repeated experiment as a sample from an infinite population with independently and identically distribution and then do inference?"

google "Dennis Cook’s envelope method"

Note that derived Y values from this method are conditional on the type of model used in order to obtain residuals. 
See rms text pdf 139 top for similar mention regarding the bootstrap.

https://rpruim.github.io/s341/S19/from-class/MathinRmd.html

https://www.math.mcgill.ca/yyang/regression/RMarkdown/example.html

https://www.tenderisthebyte.com/blog/2019/04/25/rotating-axis-labels-in-r/

https://stackoverflow.com/questions/33265467/nls-troubles-missing-value-or-an-infinity-produced-when-evaluating-the-model

This 
https://medium.com/the-quantastic-journal/uncovering-biases-in-data-with-the-kullback-leibler-divergence-57a1ee57e2ba
maybe relevant i.e. sampling with weights produces different estimates of the true density function i.e. pdf
